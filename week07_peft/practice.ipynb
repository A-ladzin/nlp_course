{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSWEcS2XKgzi"
      },
      "source": [
        "### Practice: Parameter Efficient Fine-Tuning\n",
        "In this notebook, you're gonna fine-tune large language models within limited GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7xeRF_hSKgzs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# %pip install --quiet transformers==4.34.1 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 peft==0.5.0 bitsandbytes==0.41.2.post2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import transformers\n",
        "from tqdm.auto import tqdm, trange\n",
        "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "a7adb06901b34e03893f30ecc23b97ee",
            "d94f975c69b7421f9851edeff1acbc1d",
            "7cc587f710c94a72976f67013c0d18f1",
            "7db40f2dcb2e46309b29e137eac7bba2",
            "b6013ba4a99743b3a00f7a51a366a507",
            "7220ba464c234cbba33068f18f68e7c8",
            "94aa4f70041942639c8759a9e371b80e",
            "a83adb34773a459a89ae687f328b1aa2",
            "3ee1280bc2b6439e8298f0ea8c74d30e",
            "93ed39f5849c493eaa8035bd3d11047b",
            "845a855f36124f03a30829e21df98702"
          ]
        },
        "id": "VMzFwx29Kgzu",
        "outputId": "7077979d-a419-4b4b-af7f-ff34d54697ca"
      },
      "outputs": [],
      "source": [
        "model_name = 'Enoch/llama-7b-hf'\n",
        "\n",
        "# loading Llama tokenizer ...\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# ... and the model itself\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n",
        "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n",
        "model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n",
        "# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgspB2JwSIS2"
      },
      "source": [
        "### Prompt tuning: the story of a fox (2 pts)\n",
        "\n",
        "![img](https://i.imgur.com/Ux3qQAu.png) (source: theodd1souts.fandom.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H13pYFRxQi4U",
        "outputId": "597e1af9-399a-41ab-8d8d-9c1c216d906c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output: <s>A quick brown fox jumps over the lazy dog.\n",
            "A quick\n"
          ]
        }
      ],
      "source": [
        "prompt = 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "for i in range(10):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVhZACT6SgLq"
      },
      "source": [
        "What a blatant lie! This particular fox assures you that it didn't in fact jump over the lazy dog. No, sir! The fox was just minding its own business. __Your task is to train the model to say truth: no dog was jumped over today.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r6UVDl4NEua",
        "outputId": "67ab27e0-af96-41c7-f0a9-db92e842cd80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: tensor(3.0729, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "outputs = model(**batch)\n",
        "\n",
        "next_word_logits = outputs.logits[:, :-1]\n",
        "true_next_tokens = batch['input_ids'][:, 1:]\n",
        "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "\n",
        "print(\"Loss:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amvNufS8WXa0"
      },
      "source": [
        "Except, we can't train the entire model - that would be 28GB gradients in float32. Instead, let's run [prompt tuning](https://arxiv.org/abs/2104.08691).\n",
        "\n",
        "![img](https://i.imgur.com/VwNNKnb.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "73ZOCFRZWR98"
      },
      "outputs": [],
      "source": [
        "class WordEmbeddingsWithLearnedPrompts(nn.Module):\n",
        "    \"\"\"\n",
        "    To perform prompt tuning, you will need to replace model's original word embeddings with a layer - THIS layer\n",
        "     - that inserts trainable prompts instead of the first N token embeddings. \"\"\"\n",
        "\n",
        "    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n",
        "        super().__init__()\n",
        "        self.original_word_embeddings = word_embeddings\n",
        "        self.num_prompts = num_prompts\n",
        "        self.learnable_prompts = nn.Parameter(\n",
        "            torch.randn(1, num_prompts, word_embeddings.embedding_dim), requires_grad=True)\n",
        "\n",
        "    def forward(self, input_ids: torch.LongTensor):\n",
        "        # input_ids shape: [batch_size, seq length]\n",
        "        assert input_ids.dtype == torch.int64\n",
        "        assert input_ids.shape[1] > self.num_prompts\n",
        "        assert torch.all(input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).item(), \"don't forget to prepend several BOS tokens to input_ids\"\n",
        "\n",
        "        # Your task: embed input_ids, but replace the first :num_prompts: tokens with self.learnable_prompts\n",
        "        # This is because we will prepend :num_prompts: padding tokens at the beginning\n",
        "        \n",
        "        # After you are done, you must produce a word embedding vector for each token in input_ids,\n",
        "        # except that the first :num_prompts: vectors should equal learnable_prompts;\n",
        "        # any additional vectors after first :num_prompts: ones should be embedded as usual\n",
        "        # Note: since you're dealing with trainable params, please torch.cat instead of item assignment\n",
        "\n",
        "        # <YOUR CODE HERE>\n",
        "        return torch.cat((self.learnable_prompts,self.original_word_embeddings(input_ids[...,self.num_prompts:])),dim=1)\n",
        "        # return your_outputs_with_prompts_as_per_instructions_above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_prompts = 16\n",
        "test_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxUyUU2uT2f1",
        "outputId": "9d0de5c1-162a-4a6f-92c1-1a7f41069464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looks legit!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_9456\\3577130558.py:7: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        }
      ],
      "source": [
        "\n",
        "test_input_ids = tokenizer(\"a cat say on a may\", return_tensors='pt')['input_ids'].to(device)\n",
        "\n",
        "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
        "                               dtype=torch.int64, device=device)\n",
        "test_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n",
        "\n",
        "assert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\n",
        "assert test_prompt_embeddings.shape[-1] == model.config.hidden_size\n",
        "assert torch.allclose(test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.float())\n",
        "assert torch.allclose(test_prompt_embeddings[:, num_prompts:], model.model.embed_tokens(test_input_ids).float())\n",
        "print(\"Looks legit!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbKPgfT-crqW"
      },
      "source": [
        "__Now that it works,__ let's inject learnable prompts into the main model and teach it about foxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QRe0lpREV49G"
      },
      "outputs": [],
      "source": [
        "assert isinstance(model.model.embed_tokens, nn.Embedding), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n",
        "\n",
        "model.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
        "\n",
        "opt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3gVQzgdka-Bm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: tensor(7.7649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(7.0887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(6.7318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(5.8730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(5.2210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.6203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.1079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.9123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.7183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.5747, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.4193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.1218, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.9783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.9256, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.8579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.7877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.7156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.6470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.5930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.5276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2359, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1626, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9797, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8000, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5796, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5554, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5324, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5105, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4692, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4301, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4112, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.3927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.3747, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.3574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.3407, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.3247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.3097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2578, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2362, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2261, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.1962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.1852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.1732, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.1597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.1445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.1272, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.1077, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.0861, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.0626, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.0373, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.0109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.9837, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.9563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.9292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.9027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8771, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8291, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.7856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.7654, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.7461, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.7278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.7101, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.5885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.5607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.5467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.5326, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.5183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.5039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.4893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.4744, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.4594, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.4444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.4292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.4143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3995, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3460, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2988, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2846, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2835, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2772, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2651, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1697, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1490, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Good job!\n"
          ]
        }
      ],
      "source": [
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
        "                               dtype=torch.int64, device=device)\n",
        "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
        "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
        "\n",
        "loss = torch.inf\n",
        "while loss > 0.1:\n",
        "    outputs = model(**batch)\n",
        "    next_word_logits = outputs.logits[:, num_prompts : -1, :]\n",
        "    true_next_tokens = batch['input_ids'][:, num_prompts + 1:]\n",
        "    loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    print(\"Loss:\", loss)\n",
        "\n",
        "\n",
        "# raise NotImplemented(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")\n",
        "\n",
        "\n",
        "assert loss.item() <= 0.1\n",
        "print(\"Good job!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "F7DkWHD-r1Xo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output: <s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it\n"
          ]
        }
      ],
      "source": [
        "prompt = 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
        "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
        "\n",
        "\n",
        "for i in range(15):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, num_prompts:].cpu().numpy().tolist()))\n",
        "\n",
        "# if you did everything right, the model will deny that the fox jumped over the lazy dog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEkoFNdlshv_"
      },
      "source": [
        "### Using HuggingFace PEFT (2 points)\n",
        "\n",
        "[`peft`](https://huggingface.co/docs/peft/index) is a transformer's sister library that allows you to apply various __p__arameter __e__fficient __f__ine-__t__uning methods to pre-trained transformers. The library imlements both prompt tuning, prefix tuning, as well as several adapter-based techniques under a common interface:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqEEpZm2Q4UC",
        "outputId": "2b760d0e-ac1e-4580-9472-997d1275385d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 65536\n",
            "Total parameters (excluding quantization): 3500478464\n"
          ]
        }
      ],
      "source": [
        "import peft\n",
        "assert isinstance(model.model.embed_tokens, nn.Embedding), \"please reload the model\"\n",
        "\n",
        "peft_config = peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM, num_virtual_tokens=16)\n",
        "model = peft.get_peft_model(model, peft_config)  # note: for most peft methods, this line also modifies model in-place\n",
        "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print(\"Total parameters (excluding quantization):\", sum(p.numel() for p in model.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt_encoder.default.embedding.weight torch.Size([16, 4096])\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name,param.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UW54GnzCwVpp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: tensor(7.4149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(6.9017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(6.5091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(6.0766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(5.7556, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(5.5157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(5.1172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.6599, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.2554, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.1567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.1443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.1045, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(4.0361, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.9559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.8850, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.8497, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.8244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.7583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.6455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.5088, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.3906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.3026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(3.1469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.8850, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.8050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.7475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.7153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.7125, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.7203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.7450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.7053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.6839, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.6662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.6444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.6200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.5943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.5679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.5414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.5150, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4357, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3823, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3555, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2757, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0087, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9566, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8806, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8319, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8086, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7863, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7078, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6835, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6745, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6368, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5628, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.3921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.3580, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.3251, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2379, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.2130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.1902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.1693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.1499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.1318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.1151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.0995, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.0852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.0719, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.0595, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.0477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.0365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.0256, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.0148, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.0040, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.9933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.9824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.9714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.9603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.9491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.9378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.9264, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.9149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.9035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8817, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8562, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8428, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.8071, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.7951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.7820, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.7680, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.7532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.7380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.7227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.6015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.5782, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.5527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.5250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.4968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.4705, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.4477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.4283, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.4117, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3838, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3697, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3373, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.3188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Good job!\n"
          ]
        }
      ],
      "source": [
        "# Your task: optimize the PEFT-wrapped model to achieve next token prediction loss < 0.1, but this time using PEFT\n",
        "# Please note: you no longer need to prepend PAD tokens, but you still need to skip :num_virtual_tokens: first logits.\n",
        "# Finally, generate the sentence to make sure that the model learned the truth.\n",
        "\n",
        "\n",
        "\n",
        "model.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=0).to(device)\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# raise NotImplemented(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")\n",
        "\n",
        "\n",
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "loss = torch.inf\n",
        "while loss > 0.1:\n",
        "    outputs = model(**batch)\n",
        "    next_word_logits = outputs.logits[:, num_prompts : -1, :]\n",
        "    true_next_tokens = batch['input_ids'][:, 1:]\n",
        "    loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    print(\"Loss:\", loss)\n",
        "\n",
        "\n",
        "# raise NotImplemented(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")\n",
        "\n",
        "\n",
        "assert loss.item() <= 0.1\n",
        "print(\"Good job!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71vJ9Mq7w67f"
      },
      "outputs": [],
      "source": [
        "# Feel free to structure your code as you see fit - as long as it's legible :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCkpKYjWxfhk"
      },
      "source": [
        "### Parameter-efficient finetuning with LoRA (2 points)\n",
        "\n",
        "When training on more serious tasks, you can use low-rank adapters based on the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf).\n",
        "\n",
        "The core idea is to add low-rank adapters __in parallel with existing linear layers,__ like this:\n",
        "<center><img src=\"https://i.imgur.com/6bQLNiG.png\" width=240px></center>\n",
        "\n",
        "In the original LoRA paper, the adapters were only added to attention projection matrices. However, [subsequent works](https://arxiv.org/abs/2305.14314) show that it is useful to adapt FFNs as well. But before we do any training, we need to implement the basic LoRA layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b87c54c3bed847ab933eae8175359169",
            "8aaa22971b3e416eae8394ef3b3b3f0f",
            "e9c4ba0d262c4b76baa00532e209b92f",
            "e6f9064e6ec545debe2e90163f4c712c",
            "6aad5b046def4a7db1048434e874b5d5",
            "dba48e929a2e43ec8e4bb3d4b32475ca",
            "530d66e4732b4d5486165654415bd2dc",
            "2bd4b6acd8004c1e98c064708108938e",
            "09b07105e4f54d2bb5e6cc1c1f1a9c8e",
            "923869a5864c4d3d80fb76c99fff24e2",
            "25e1f3d72230485c9b84cae4f685a69a"
          ]
        },
        "id": "8zundaSzx90r",
        "outputId": "3faf7150-7685-4089-cf58-e03e4fce8bc6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100%|██████████| 33/33 [00:24<00:00,  1.37it/s]\n"
          ]
        }
      ],
      "source": [
        "# re-load the model to remove any previous PEFT tuners\n",
        "model_name = 'Enoch/llama-7b-hf'\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n",
        "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MJ_hq4fwyPVR"
      },
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n",
        "    def __init__(self, module: nn.Linear, rank: int):\n",
        "        super().__init__()\n",
        "        self.module = module  # pre-trained (frozen) linear layer\n",
        "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
        "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
        "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)\n",
        "        #  <YOUR CODE HERE>\n",
        "        return self.module(input)+input@self.adapter_A@self.adapter_B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTzOs65JydcS",
        "outputId": "e07177c9-2f2b-432a-8a97-9e507df166bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# test your implementation\n",
        "test_linear = nn.Linear(128, 128)\n",
        "test_linear.weight.data[...] = torch.eye(128)\n",
        "test_adapter = LoRALayer(test_linear, rank=8)\n",
        "\n",
        "assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n",
        "\n",
        "test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n",
        "test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n",
        "test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n",
        "\n",
        "dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\n",
        "assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n",
        "dummy_loss.backward()\n",
        "assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n",
        "assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n",
        "assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n",
        "# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n",
        "del dummy_loss, test_linear, test_adapter\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tajVTsvLulB6"
      },
      "source": [
        "### Apply LoRA to the model\n",
        "\n",
        "The code below applies LoRA adapters on top of Q/K/V linear layers in Llama attention. You may also choose to modify other layers:\n",
        "* self_attn.o_proj - attention output projection\n",
        "* mlp.up_proj, mlp.gate_proj, mlp.down_proj - transformer feedforward layers\n",
        "* lm_head - output LM head\n",
        "\n",
        "__Note:__ please scroll down for the homework task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "davyUVEwulB6"
      },
      "outputs": [],
      "source": [
        "lora_rank = 8\n",
        "\n",
        "for name, module in model.model.layers.named_modules():\n",
        "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
        "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
        "\n",
        "assert sum(isinstance(module, LoRALayer) for module in model.modules()) == 96  # for Llama-7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWzfvc0EulB6",
        "outputId": "b432afe7-08b9-4cb2-c6ac-01f85352a689",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_12972\\614644202.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float32):\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grad check successful, well done!\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "batch = tokenizer(\"This model wants to share its greatest secret:\", return_tensors='pt', return_token_type_ids=False)\n",
        "# test a single training step, make sure we get meaningful gradients\n",
        "with torch.cuda.amp.autocast(dtype=torch.float32):\n",
        "    out = model.forward(**batch)\n",
        "    (out.logits.norm() / 100).backward()\n",
        "\n",
        "for i, module in enumerate(model.modules()):\n",
        "    if isinstance(module, LoRALayer):\n",
        "        assert module.adapter_B.grad is not None\n",
        "        assert module.adapter_B.grad.norm().item() > 0\n",
        "\n",
        "model.zero_grad(set_to_none=True)\n",
        "print(\"Grad check successful, well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjIJ1vkUulB7"
      },
      "source": [
        "### (example) How to train your model\n",
        "\n",
        "The example below shows how to train the LoRA adapters on a dummy dataset. You will need to run a _similar_ training task later.\n",
        "\n",
        "__Note:__ please scroll down for the homework task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a868f8a190f74df2a99a50e2ca9a7cb3",
            "895c44b30fd24b8b9bbedc631a7934ec",
            "92f3ee2ed68c4defbed2fe9bef0f20b2",
            "3321598939fd4d76957155f58097fadd",
            "f0fe9da3411840ef8d7eff80883cb8e9",
            "8ad2b69a25304e5f903f2fd43b538340",
            "7aea6cd9a18b4dd9b40bbe65fb0b9069",
            "f72b2d490b04440b800ac3c8ab05e625",
            "c6ddf10ea9ef499f917c81fde3e63cd2",
            "31c4ef0dab98410d88891a2c27fdb5c1",
            "54b23e64bbc444b4abc3f25832e3677d"
          ]
        },
        "id": "r9mIpntHulB8",
        "outputId": "21b0c176-b6b8-4b4e-c193-c4c8c61a3bf7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
            "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
            "the same time. Both libraries are known to be incompatible and this\n",
            "can cause random crashes or deadlocks on Linux when loaded in the\n",
            "same Python program.\n",
            "Using threadpoolctl may cause crashes or deadlocks. For more\n",
            "information and possible workarounds, please see\n",
            "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
            "\n",
            "  warnings.warn(msg, RuntimeWarning)\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "  1%|          | 1/100 [00:01<02:51,  1.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.2675, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 2/100 [00:02<02:03,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3792, 'grad_norm': 22.897201538085938, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 3/100 [00:03<01:33,  1.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.4696, 'grad_norm': inf, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 4/100 [00:03<01:19,  1.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.4339, 'grad_norm': inf, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 5/100 [00:04<01:13,  1.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8614, 'grad_norm': 50.877445220947266, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.31}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 6/100 [00:05<01:07,  1.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.6614, 'grad_norm': 65.26477813720703, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.38}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 7/100 [00:05<01:04,  1.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.8409, 'grad_norm': inf, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 8/100 [00:06<01:01,  1.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.2711, 'grad_norm': 115.74801635742188, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 9/100 [00:07<01:00,  1.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5583, 'grad_norm': 24.011795043945312, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 10/100 [00:07<00:58,  1.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.2584, 'grad_norm': 66.3298568725586, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.62}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 11/100 [00:09<01:23,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3658, 'grad_norm': 35.30513000488281, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.69}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 12/100 [00:09<01:14,  1.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7525, 'grad_norm': 51.85884475708008, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.75}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 13/100 [00:10<01:07,  1.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0483, 'grad_norm': 81.05425262451172, 'learning_rate': 7.2e-06, 'epoch': 0.81}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 14/100 [00:11<01:03,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5217, 'grad_norm': 19.20269203186035, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.88}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 15/100 [00:11<01:00,  1.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.5557, 'grad_norm': 61.99456787109375, 'learning_rate': 8.8e-06, 'epoch': 0.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 16/100 [00:12<00:57,  1.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.2183, 'grad_norm': 73.17085266113281, 'learning_rate': 9.600000000000001e-06, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 17/100 [00:13<00:55,  1.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0534, 'grad_norm': 79.35118865966797, 'learning_rate': 1.04e-05, 'epoch': 1.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 18/100 [00:13<00:53,  1.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7774, 'grad_norm': 40.017433166503906, 'learning_rate': 1.1200000000000001e-05, 'epoch': 1.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 19/100 [00:15<01:15,  1.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2333, 'grad_norm': 7.151697635650635, 'learning_rate': 1.2e-05, 'epoch': 1.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 20/100 [00:15<01:07,  1.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9655, 'grad_norm': 23.961135864257812, 'learning_rate': 1.2800000000000001e-05, 'epoch': 1.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 21/100 [00:16<01:01,  1.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8837, 'grad_norm': 29.00138282775879, 'learning_rate': 1.3600000000000002e-05, 'epoch': 1.31}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 22/100 [00:17<00:57,  1.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.2007, 'grad_norm': 22.821115493774414, 'learning_rate': 1.44e-05, 'epoch': 1.38}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 23/100 [00:17<00:54,  1.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2888, 'grad_norm': 9.484503746032715, 'learning_rate': 1.52e-05, 'epoch': 1.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 24/100 [00:18<00:52,  1.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0974, 'grad_norm': 38.50327682495117, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 25/100 [00:19<00:49,  1.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6387, 'grad_norm': 21.297584533691406, 'learning_rate': 1.6800000000000002e-05, 'epoch': 1.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 26/100 [00:19<00:49,  1.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3019, 'grad_norm': 5.989992618560791, 'learning_rate': 1.76e-05, 'epoch': 1.62}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 27/100 [00:20<00:48,  1.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7107, 'grad_norm': 13.446832656860352, 'learning_rate': 1.84e-05, 'epoch': 1.69}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 28/100 [00:21<00:46,  1.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7633, 'grad_norm': 10.066360473632812, 'learning_rate': 1.9200000000000003e-05, 'epoch': 1.75}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▉       | 29/100 [00:21<00:45,  1.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8045, 'grad_norm': 14.253836631774902, 'learning_rate': 2e-05, 'epoch': 1.81}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 30/100 [00:22<00:47,  1.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1847, 'grad_norm': 3.207367181777954, 'learning_rate': 2.08e-05, 'epoch': 1.88}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|███       | 31/100 [00:23<00:45,  1.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8651, 'grad_norm': 11.973697662353516, 'learning_rate': 2.16e-05, 'epoch': 1.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 32/100 [00:23<00:44,  1.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7201, 'grad_norm': 11.341193199157715, 'learning_rate': 2.2400000000000002e-05, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 33/100 [00:24<00:42,  1.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8389, 'grad_norm': 13.482047080993652, 'learning_rate': 2.32e-05, 'epoch': 2.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 34/100 [00:24<00:41,  1.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4408, 'grad_norm': 6.821287155151367, 'learning_rate': 2.4e-05, 'epoch': 2.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 35/100 [00:25<00:40,  1.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6397, 'grad_norm': 8.455562591552734, 'learning_rate': 2.48e-05, 'epoch': 2.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 36/100 [00:26<00:39,  1.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6209, 'grad_norm': 10.973292350769043, 'learning_rate': 2.5600000000000002e-05, 'epoch': 2.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 37/100 [00:26<00:39,  1.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2274, 'grad_norm': 3.2815420627593994, 'learning_rate': 2.64e-05, 'epoch': 2.31}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 38/100 [00:27<00:40,  1.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.235, 'grad_norm': 5.127390384674072, 'learning_rate': 2.7200000000000004e-05, 'epoch': 2.38}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 39/100 [00:28<00:38,  1.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3945, 'grad_norm': 9.673784255981445, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 40/100 [00:28<00:38,  1.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3509, 'grad_norm': 6.50076150894165, 'learning_rate': 2.88e-05, 'epoch': 2.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████      | 41/100 [00:29<00:37,  1.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5237, 'grad_norm': 7.917452335357666, 'learning_rate': 2.96e-05, 'epoch': 2.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 42/100 [00:30<00:53,  1.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1061, 'grad_norm': 6.26846981048584, 'learning_rate': 3.04e-05, 'epoch': 2.62}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 43/100 [00:31<00:47,  1.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5102, 'grad_norm': 14.553844451904297, 'learning_rate': 3.12e-05, 'epoch': 2.69}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 44/100 [00:32<00:43,  1.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6829, 'grad_norm': 14.47188949584961, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.75}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 45/100 [00:32<00:40,  1.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3599, 'grad_norm': 7.167393207550049, 'learning_rate': 3.2800000000000004e-05, 'epoch': 2.81}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 46/100 [00:33<00:40,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1262, 'grad_norm': 2.4738872051239014, 'learning_rate': 3.3600000000000004e-05, 'epoch': 2.88}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 47/100 [00:34<00:37,  1.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3515, 'grad_norm': 7.8372483253479, 'learning_rate': 3.4399999999999996e-05, 'epoch': 2.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 48/100 [00:34<00:35,  1.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2339, 'grad_norm': 4.4843597412109375, 'learning_rate': 3.52e-05, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 49/100 [00:35<00:34,  1.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2522, 'grad_norm': 5.351723670959473, 'learning_rate': 3.6e-05, 'epoch': 3.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 50/100 [00:36<00:32,  1.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4073, 'grad_norm': 9.736820220947266, 'learning_rate': 3.68e-05, 'epoch': 3.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|█████     | 51/100 [00:36<00:32,  1.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2052, 'grad_norm': 3.454028367996216, 'learning_rate': 3.76e-05, 'epoch': 3.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 52/100 [00:37<00:30,  1.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2801, 'grad_norm': 5.898352146148682, 'learning_rate': 3.8400000000000005e-05, 'epoch': 3.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 53/100 [00:38<00:29,  1.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1955, 'grad_norm': 5.010529041290283, 'learning_rate': 3.9200000000000004e-05, 'epoch': 3.31}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|█████▍    | 54/100 [00:38<00:28,  1.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.507, 'grad_norm': 10.909318923950195, 'learning_rate': 4e-05, 'epoch': 3.38}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 55/100 [00:39<00:29,  1.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1322, 'grad_norm': 4.1200103759765625, 'learning_rate': 4.08e-05, 'epoch': 3.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 56/100 [00:39<00:27,  1.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4093, 'grad_norm': 8.369059562683105, 'learning_rate': 4.16e-05, 'epoch': 3.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 57/100 [00:40<00:26,  1.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2737, 'grad_norm': 7.272174835205078, 'learning_rate': 4.24e-05, 'epoch': 3.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 58/100 [00:41<00:26,  1.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3559, 'grad_norm': 7.681825160980225, 'learning_rate': 4.32e-05, 'epoch': 3.62}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|█████▉    | 59/100 [00:41<00:25,  1.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4233, 'grad_norm': 10.924784660339355, 'learning_rate': 4.4000000000000006e-05, 'epoch': 3.69}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 60/100 [00:42<00:25,  1.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4716, 'grad_norm': 11.375064849853516, 'learning_rate': 4.4800000000000005e-05, 'epoch': 3.75}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████    | 61/100 [00:43<00:24,  1.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2036, 'grad_norm': 6.248593807220459, 'learning_rate': 4.5600000000000004e-05, 'epoch': 3.81}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 62/100 [00:43<00:25,  1.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0624, 'grad_norm': 0.9719113111495972, 'learning_rate': 4.64e-05, 'epoch': 3.88}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 63/100 [00:45<00:35,  1.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0715, 'grad_norm': 1.8064085245132446, 'learning_rate': 4.72e-05, 'epoch': 3.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 64/100 [00:46<00:31,  1.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5446, 'grad_norm': 13.089944839477539, 'learning_rate': 4.8e-05, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 65/100 [00:47<00:38,  1.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0429, 'grad_norm': 0.8484355807304382, 'learning_rate': 4.88e-05, 'epoch': 4.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|██████▌   | 66/100 [00:48<00:32,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2275, 'grad_norm': 5.87984037399292, 'learning_rate': 4.96e-05, 'epoch': 4.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 67/100 [00:49<00:28,  1.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2419, 'grad_norm': 5.233254909515381, 'learning_rate': 5.0400000000000005e-05, 'epoch': 4.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 68/100 [00:49<00:25,  1.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.251, 'grad_norm': 5.404520034790039, 'learning_rate': 5.1200000000000004e-05, 'epoch': 4.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 69/100 [00:50<00:23,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2315, 'grad_norm': 5.2607927322387695, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.31}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 70/100 [00:50<00:21,  1.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3012, 'grad_norm': 5.861220836639404, 'learning_rate': 5.28e-05, 'epoch': 4.38}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████   | 71/100 [00:51<00:20,  1.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.214, 'grad_norm': 4.143435955047607, 'learning_rate': 5.360000000000001e-05, 'epoch': 4.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 72/100 [00:52<00:19,  1.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0958, 'grad_norm': 1.9838590621948242, 'learning_rate': 5.440000000000001e-05, 'epoch': 4.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 73/100 [00:52<00:18,  1.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3001, 'grad_norm': 9.849335670471191, 'learning_rate': 5.520000000000001e-05, 'epoch': 4.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 74/100 [00:53<00:17,  1.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.167, 'grad_norm': 5.37661600112915, 'learning_rate': 5.6000000000000006e-05, 'epoch': 4.62}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▌  | 75/100 [00:54<00:16,  1.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.106, 'grad_norm': 3.3722434043884277, 'learning_rate': 5.68e-05, 'epoch': 4.69}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 76/100 [00:54<00:16,  1.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1284, 'grad_norm': 2.5554332733154297, 'learning_rate': 5.76e-05, 'epoch': 4.75}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 77/100 [00:55<00:15,  1.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2455, 'grad_norm': 6.484170913696289, 'learning_rate': 5.8399999999999997e-05, 'epoch': 4.81}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 78/100 [00:56<00:14,  1.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3504, 'grad_norm': 8.825346946716309, 'learning_rate': 5.92e-05, 'epoch': 4.88}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 79%|███████▉  | 79/100 [00:56<00:13,  1.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4538, 'grad_norm': 13.929696083068848, 'learning_rate': 6e-05, 'epoch': 4.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 80/100 [00:57<00:12,  1.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3354, 'grad_norm': 6.196852684020996, 'learning_rate': 6.08e-05, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 81/100 [00:58<00:12,  1.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1673, 'grad_norm': 3.9931299686431885, 'learning_rate': 6.16e-05, 'epoch': 5.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▏ | 82/100 [00:58<00:11,  1.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.201, 'grad_norm': 4.48912239074707, 'learning_rate': 6.24e-05, 'epoch': 5.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 83/100 [00:59<00:11,  1.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0606, 'grad_norm': 1.1160122156143188, 'learning_rate': 6.32e-05, 'epoch': 5.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 84/100 [01:00<00:11,  1.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1459, 'grad_norm': 4.275103569030762, 'learning_rate': 6.400000000000001e-05, 'epoch': 5.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 85/100 [01:00<00:10,  1.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.126, 'grad_norm': 2.634509801864624, 'learning_rate': 6.48e-05, 'epoch': 5.31}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 86/100 [01:01<00:09,  1.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1028, 'grad_norm': 2.101379871368408, 'learning_rate': 6.560000000000001e-05, 'epoch': 5.38}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 87/100 [01:02<00:08,  1.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3793, 'grad_norm': 9.356125831604004, 'learning_rate': 6.64e-05, 'epoch': 5.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 88/100 [01:02<00:08,  1.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1538, 'grad_norm': 3.2313811779022217, 'learning_rate': 6.720000000000001e-05, 'epoch': 5.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%|████████▉ | 89/100 [01:03<00:07,  1.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1508, 'grad_norm': 2.8819150924682617, 'learning_rate': 6.800000000000001e-05, 'epoch': 5.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 90/100 [01:05<00:09,  1.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0248, 'grad_norm': 0.7685672044754028, 'learning_rate': 6.879999999999999e-05, 'epoch': 5.62}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 91/100 [01:05<00:07,  1.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2027, 'grad_norm': 4.6777729988098145, 'learning_rate': 6.96e-05, 'epoch': 5.69}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 92/100 [01:06<00:06,  1.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0802, 'grad_norm': 1.7940375804901123, 'learning_rate': 7.04e-05, 'epoch': 5.75}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 93/100 [01:07<00:05,  1.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2069, 'grad_norm': 5.598191738128662, 'learning_rate': 7.12e-05, 'epoch': 5.81}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|█████████▍| 94/100 [01:07<00:04,  1.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2407, 'grad_norm': 5.548616886138916, 'learning_rate': 7.2e-05, 'epoch': 5.88}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 95/100 [01:08<00:03,  1.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2267, 'grad_norm': 7.095282077789307, 'learning_rate': 7.280000000000001e-05, 'epoch': 5.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 96/100 [01:09<00:02,  1.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4176, 'grad_norm': 10.833306312561035, 'learning_rate': 7.36e-05, 'epoch': 6.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 97/100 [01:09<00:01,  1.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2937, 'grad_norm': 8.432310104370117, 'learning_rate': 7.44e-05, 'epoch': 6.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 98/100 [01:11<00:01,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.018, 'grad_norm': 0.3951903283596039, 'learning_rate': 7.52e-05, 'epoch': 6.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 99/100 [01:12<00:00,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0892, 'grad_norm': 2.6352131366729736, 'learning_rate': 7.6e-05, 'epoch': 6.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:12<00:00,  1.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1019, 'grad_norm': 2.6737139225006104, 'learning_rate': 7.680000000000001e-05, 'epoch': 6.25}\n",
            "{'train_runtime': 72.7974, 'train_samples_per_second': 2.747, 'train_steps_per_second': 1.374, 'train_loss': 0.4747335730306804, 'epoch': 6.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=0.4747335730306804, metrics={'train_runtime': 72.7974, 'train_samples_per_second': 2.747, 'train_steps_per_second': 1.374, 'total_flos': 635027169632256.0, 'train_loss': 0.4747335730306804, 'epoch': 6.25})"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checking if the model can learn. Change max_steps for proper training\n",
        "import datasets\n",
        "data = datasets.load_dataset(\"Abirate/english_quotes\", split=\"train[:32]\") # 32 lines\n",
        "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n",
        "model._hf_peft_config_loaded = True  # silence a warning from HF trainer\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model, train_dataset=data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=2, gradient_accumulation_steps=1,\n",
        "        # note: if you want larger batch size, increase gradient_accumulation_steps\n",
        "        warmup_steps=250, max_steps=100, learning_rate=2e-4, fp16=True,\n",
        "        logging_steps=1, output_dir='outputs', report_to=None),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# NOTE: this is just an example! you do not have to wait for this progressbar to finish :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQUlqoEAulB8"
      },
      "source": [
        "### Final task: *actually* train the model (4 points)\n",
        "\n",
        "Your task is to fine-tune the model to _generate python code_. Please use the above examples for inspiration. More specifically,\n",
        "\n",
        "* __dataset:__ use [codeparrot-clean](https://huggingface.co/datasets/codeparrot/codeparrot-clean) or any other data containing python code. Since you do not need much data for this excercise, it is enough to use just shorter validation subset of `codeparrots`\n",
        "* __preprocessing:__ select python code based on file extentions (.py)  (may skip in case of codeparrot - it is 100% python)\n",
        "* __short lines:__ please take the first 512 characters of each line\n",
        "* __adapter type:__ please use LoRA as defined above __plus at least one of:__\n",
        "   - extra adapter on lm_head\n",
        "   - extra adapter on MLP components (mlp.*)\n",
        "   - trainable input embeddings (requires tweaking memory usage)\n",
        "\n",
        "* __training:__ you do not have to train to convergence. If all goes well, your model should `.generate` code after 500 steps. Please use batch size of at least 4 (4 x 1 x 512 tokens) using `gradient_accumulation_steps=4`.\n",
        "\n",
        "\n",
        "Note: the peft library also has LoRA implementation. However, we ask that for this assignment you show at least one complete training run with your own LoRA code.\n",
        "\n",
        "__Alternative assignment:__ Instead of doing python code, feel free to substitute the task with any other dataset, e.g. your favorite artist or podcast, as long as it's ethical. If you choose your own task, please show examples of what your model learned - or did not learn, akin to the code examples below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_LfFWSYhulB8"
      },
      "outputs": [],
      "source": [
        "prompts =  ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"codeparrot/codeparrot-clean\",split='train')\n",
        "\n",
        "# <A WHOLE LOT OF YOUR CODE>\n",
        "# generate baseline samples with the selected prompts before finetuning\n",
        "# please feel free to use transformers.Trainer (as above) or your custom training code\n",
        "# after the training concludes, please show examples of text generated by your model. It is expected to look like Python code fragments\n",
        "# print the generation examples nicely (suggestion: use pandas or HTML) for easier comparison\n",
        "# note: your LoRA-enhanced model can run generation the same way as the non-trained model (above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = ds.train_test_split(test_size = 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = ds['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Map: 100%|██████████| 2000/2000 [00:19<00:00, 101.74 examples/s]\n"
          ]
        }
      ],
      "source": [
        "trunc_size = 512\n",
        "data = data.map(lambda samples: tokenizer(samples['content'],max_length=512), batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "lora_rank = 8\n",
        "for name, module in model.model.layers.named_modules():\n",
        "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
        "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
        "        # module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
        "        module.mlp.gate_proj = LoRALayer(module.mlp.gate_proj,rank = lora_rank)\n",
        "        # module.mlp.up_proj = LoRALayer(module.mlp.up_proj,rank = lora_rank)\n",
        "        module.mlp.down_proj = LoRALayer(module.mlp.down_proj,rank = lora_rank)\n",
        "model.lm_head = LoRALayer(model.lm_head,rank = lora_rank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "class Trainer(transformers.Trainer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model= None,\n",
        "        args= None,\n",
        "        data_collator= None,\n",
        "        train_dataset= None,\n",
        "        eval_dataset= None,\n",
        "        tokenizer= None,\n",
        "        model_init= None,\n",
        "        compute_metrics = None,\n",
        "        callbacks = None,\n",
        "        optimizers = (None, None),\n",
        "        preprocess_logits_for_metrics= None,\n",
        "    ):\n",
        "        super().__init__(\n",
        "        model= model,\n",
        "        args= args,\n",
        "        data_collator= data_collator,\n",
        "        train_dataset= train_dataset,\n",
        "        eval_dataset= eval_dataset,\n",
        "        tokenizer= tokenizer,\n",
        "        model_init= model_init,\n",
        "        compute_metrics = compute_metrics,\n",
        "        callbacks = callbacks,\n",
        "        optimizers = optimizers,\n",
        "        preprocess_logits_for_metrics= preprocess_logits_for_metrics)\n",
        "\n",
        "    def _save(self, output_dir = None, state_dict=None):\n",
        "        os.makedirs(output_dir)\n",
        "        for name,param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                torch.save(param,os.path.join(output_dir,name))\n",
        "        if self.tokenizer is not None:\n",
        "            self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "model._hf_peft_config_loaded = True\n",
        "trainer = Trainer(\n",
        "    model=model, train_dataset=data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4, gradient_accumulation_steps=4,\n",
        "        # note: if you want larger batch size, increase gradient_accumulation_steps\n",
        "        warmup_steps=100,max_steps = 1000, learning_rate=5e-4, fp16=True,\n",
        "        logging_steps=10, output_dir='checkpoint', report_to=None,weight_decay=1e-6,dataloader_num_workers=4),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "  1%|          | 10/1000 [01:56<2:59:09, 10.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0908, 'grad_norm': 2.0334601402282715, 'learning_rate': 5e-05, 'epoch': 0.08}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 20/1000 [03:43<2:54:07, 10.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0224, 'grad_norm': 1.42673659324646, 'learning_rate': 0.0001, 'epoch': 0.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 30/1000 [05:29<2:51:31, 10.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9909, 'grad_norm': 0.9783928990364075, 'learning_rate': 0.00015, 'epoch': 0.24}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 40/1000 [07:15<2:49:45, 10.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9625, 'grad_norm': 1.1400786638259888, 'learning_rate': 0.0002, 'epoch': 0.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 50/1000 [09:01<2:47:55, 10.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0051, 'grad_norm': 0.9666153192520142, 'learning_rate': 0.00025, 'epoch': 0.4}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 60/1000 [10:47<2:46:19, 10.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9749, 'grad_norm': 0.9108871221542358, 'learning_rate': 0.0003, 'epoch': 0.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 70/1000 [12:33<2:44:21, 10.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0309, 'grad_norm': 0.7797431945800781, 'learning_rate': 0.00035, 'epoch': 0.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 80/1000 [14:19<2:42:11, 10.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.04, 'grad_norm': 0.8447738289833069, 'learning_rate': 0.0004, 'epoch': 0.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 90/1000 [16:06<2:40:57, 10.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0198, 'grad_norm': 0.7444767355918884, 'learning_rate': 0.00045000000000000004, 'epoch': 0.72}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 100/1000 [17:52<2:38:56, 10.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0687, 'grad_norm': 0.7505731582641602, 'learning_rate': 0.0005, 'epoch': 0.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 110/1000 [19:37<2:36:10, 10.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0129, 'grad_norm': 0.7740360498428345, 'learning_rate': 0.0004944444444444445, 'epoch': 0.88}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 120/1000 [21:24<2:36:02, 10.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0299, 'grad_norm': 0.710003137588501, 'learning_rate': 0.0004888888888888889, 'epoch': 0.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 130/1000 [23:19<2:42:58, 11.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9099, 'grad_norm': 0.7313244938850403, 'learning_rate': 0.00048333333333333334, 'epoch': 1.04}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 140/1000 [25:05<2:32:34, 10.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8659, 'grad_norm': 0.7424031496047974, 'learning_rate': 0.0004777777777777778, 'epoch': 1.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 150/1000 [26:52<2:32:37, 10.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7681, 'grad_norm': 0.6349002122879028, 'learning_rate': 0.00047222222222222224, 'epoch': 1.2}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 160/1000 [28:41<2:32:50, 10.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8848, 'grad_norm': 0.707771897315979, 'learning_rate': 0.00046666666666666666, 'epoch': 1.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 170/1000 [30:29<2:29:29, 10.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8785, 'grad_norm': 0.6342661380767822, 'learning_rate': 0.00046111111111111114, 'epoch': 1.36}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 180/1000 [32:18<2:28:10, 10.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.857, 'grad_norm': 0.6701384782791138, 'learning_rate': 0.00045555555555555556, 'epoch': 1.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 190/1000 [34:07<2:27:32, 10.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8841, 'grad_norm': 0.7401642799377441, 'learning_rate': 0.00045000000000000004, 'epoch': 1.52}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 200/1000 [35:54<2:22:43, 10.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8583, 'grad_norm': 0.621030867099762, 'learning_rate': 0.0004444444444444444, 'epoch': 1.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 210/1000 [37:40<2:19:17, 10.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8933, 'grad_norm': 0.7309430241584778, 'learning_rate': 0.0004388888888888889, 'epoch': 1.68}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 220/1000 [39:27<2:18:36, 10.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8216, 'grad_norm': 0.7145268321037292, 'learning_rate': 0.00043333333333333337, 'epoch': 1.76}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 230/1000 [41:13<2:16:02, 10.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8952, 'grad_norm': 0.6400405168533325, 'learning_rate': 0.0004277777777777778, 'epoch': 1.84}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 240/1000 [42:59<2:14:44, 10.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9221, 'grad_norm': 0.674019455909729, 'learning_rate': 0.0004222222222222222, 'epoch': 1.92}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 250/1000 [44:48<2:18:11, 11.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8457, 'grad_norm': 0.5938775539398193, 'learning_rate': 0.0004166666666666667, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 260/1000 [46:46<2:15:58, 11.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.674, 'grad_norm': 0.9442212581634521, 'learning_rate': 0.0004111111111111111, 'epoch': 2.08}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 270/1000 [48:36<2:13:39, 10.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6119, 'grad_norm': 0.5806010961532593, 'learning_rate': 0.00040555555555555554, 'epoch': 2.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 280/1000 [50:24<2:11:41, 10.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6319, 'grad_norm': 0.6522210836410522, 'learning_rate': 0.0004, 'epoch': 2.24}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▉       | 290/1000 [52:14<2:10:19, 11.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6622, 'grad_norm': 0.7228484749794006, 'learning_rate': 0.00039444444444444444, 'epoch': 2.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 300/1000 [54:04<2:07:41, 10.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6375, 'grad_norm': 0.6759237051010132, 'learning_rate': 0.0003888888888888889, 'epoch': 2.4}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|███       | 310/1000 [55:54<2:06:46, 11.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6489, 'grad_norm': 0.7484761476516724, 'learning_rate': 0.00038333333333333334, 'epoch': 2.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 320/1000 [57:43<2:03:10, 10.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.638, 'grad_norm': 0.6646093726158142, 'learning_rate': 0.00037777777777777777, 'epoch': 2.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 330/1000 [59:33<2:03:02, 11.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6336, 'grad_norm': 0.692302942276001, 'learning_rate': 0.00037222222222222225, 'epoch': 2.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 340/1000 [1:01:23<2:01:42, 11.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6916, 'grad_norm': 0.6925787329673767, 'learning_rate': 0.00036666666666666667, 'epoch': 2.72}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 350/1000 [1:03:12<1:58:10, 10.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6382, 'grad_norm': 0.7495136857032776, 'learning_rate': 0.0003611111111111111, 'epoch': 2.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 360/1000 [1:05:00<1:54:30, 10.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6964, 'grad_norm': 0.7156454920768738, 'learning_rate': 0.00035555555555555557, 'epoch': 2.88}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 370/1000 [1:06:46<1:51:26, 10.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6893, 'grad_norm': 0.6420483589172363, 'learning_rate': 0.00035, 'epoch': 2.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 380/1000 [1:08:41<1:56:04, 11.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5461, 'grad_norm': 0.6342954039573669, 'learning_rate': 0.0003444444444444445, 'epoch': 3.04}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 390/1000 [1:10:27<1:48:14, 10.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.422, 'grad_norm': 0.670333981513977, 'learning_rate': 0.0003388888888888889, 'epoch': 3.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 400/1000 [1:12:14<1:46:09, 10.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.428, 'grad_norm': 0.7301662564277649, 'learning_rate': 0.0003333333333333333, 'epoch': 3.2}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████      | 410/1000 [1:14:00<1:44:18, 10.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4641, 'grad_norm': 0.7061643600463867, 'learning_rate': 0.0003277777777777778, 'epoch': 3.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 420/1000 [1:15:46<1:42:37, 10.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4634, 'grad_norm': 0.6925646662712097, 'learning_rate': 0.0003222222222222222, 'epoch': 3.36}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 430/1000 [1:17:33<1:40:42, 10.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4439, 'grad_norm': 0.7407236695289612, 'learning_rate': 0.00031666666666666665, 'epoch': 3.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 440/1000 [1:19:19<1:38:49, 10.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4533, 'grad_norm': 0.8351072669029236, 'learning_rate': 0.0003111111111111111, 'epoch': 3.52}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 450/1000 [1:21:05<1:36:51, 10.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4746, 'grad_norm': 0.7182316184043884, 'learning_rate': 0.0003055555555555556, 'epoch': 3.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 460/1000 [1:22:51<1:35:09, 10.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.49, 'grad_norm': 0.7873105406761169, 'learning_rate': 0.0003, 'epoch': 3.68}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 470/1000 [1:24:37<1:34:00, 10.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4814, 'grad_norm': 0.783353328704834, 'learning_rate': 0.00029444444444444445, 'epoch': 3.76}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 480/1000 [1:26:23<1:31:28, 10.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4593, 'grad_norm': 0.7850754261016846, 'learning_rate': 0.0002888888888888889, 'epoch': 3.84}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 490/1000 [1:28:09<1:30:05, 10.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.524, 'grad_norm': 0.7659808397293091, 'learning_rate': 0.00028333333333333335, 'epoch': 3.92}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 500/1000 [1:29:55<1:29:34, 10.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4921, 'grad_norm': 0.7772588133811951, 'learning_rate': 0.0002777777777777778, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            " 51%|█████     | 510/1000 [1:31:50<1:27:11, 10.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2829, 'grad_norm': 0.9424892067909241, 'learning_rate': 0.0002722222222222222, 'epoch': 4.08}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 520/1000 [1:33:35<1:24:42, 10.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2854, 'grad_norm': 0.6861646175384521, 'learning_rate': 0.0002666666666666667, 'epoch': 4.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 530/1000 [1:35:23<1:23:31, 10.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2822, 'grad_norm': 0.7612541317939758, 'learning_rate': 0.00026111111111111116, 'epoch': 4.24}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|█████▍    | 540/1000 [1:37:09<1:21:39, 10.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2738, 'grad_norm': 0.7842260599136353, 'learning_rate': 0.00025555555555555553, 'epoch': 4.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 550/1000 [1:38:55<1:19:36, 10.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3066, 'grad_norm': 0.8865699172019958, 'learning_rate': 0.00025, 'epoch': 4.4}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 560/1000 [1:40:41<1:17:48, 10.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.305, 'grad_norm': 0.7283883094787598, 'learning_rate': 0.00024444444444444443, 'epoch': 4.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 570/1000 [1:42:28<1:16:29, 10.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3022, 'grad_norm': 0.6941735148429871, 'learning_rate': 0.0002388888888888889, 'epoch': 4.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 580/1000 [1:44:14<1:14:29, 10.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2922, 'grad_norm': 0.8083443641662598, 'learning_rate': 0.00023333333333333333, 'epoch': 4.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|█████▉    | 590/1000 [1:46:00<1:12:40, 10.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2904, 'grad_norm': 0.864305853843689, 'learning_rate': 0.00022777777777777778, 'epoch': 4.72}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 600/1000 [1:47:46<1:10:19, 10.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3136, 'grad_norm': 0.7143957018852234, 'learning_rate': 0.0002222222222222222, 'epoch': 4.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████    | 610/1000 [1:49:32<1:09:02, 10.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3178, 'grad_norm': 0.8186770677566528, 'learning_rate': 0.00021666666666666668, 'epoch': 4.88}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 620/1000 [1:51:17<1:06:53, 10.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3338, 'grad_norm': 0.8214730024337769, 'learning_rate': 0.0002111111111111111, 'epoch': 4.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 630/1000 [1:53:12<1:09:00, 11.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2422, 'grad_norm': 0.5681492686271667, 'learning_rate': 0.00020555555555555556, 'epoch': 5.04}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 640/1000 [1:54:59<1:03:43, 10.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.17, 'grad_norm': 0.7656020522117615, 'learning_rate': 0.0002, 'epoch': 5.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 650/1000 [1:56:45<1:01:43, 10.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1756, 'grad_norm': 0.6583871841430664, 'learning_rate': 0.00019444444444444446, 'epoch': 5.2}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|██████▌   | 660/1000 [1:58:32<1:01:14, 10.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.167, 'grad_norm': 0.7142757773399353, 'learning_rate': 0.00018888888888888888, 'epoch': 5.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 670/1000 [2:00:19<58:40, 10.67s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1654, 'grad_norm': 0.7140992879867554, 'learning_rate': 0.00018333333333333334, 'epoch': 5.36}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 680/1000 [2:02:05<56:58, 10.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1786, 'grad_norm': 0.7163277864456177, 'learning_rate': 0.00017777777777777779, 'epoch': 5.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 690/1000 [2:03:52<55:09, 10.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1648, 'grad_norm': 0.6141169667243958, 'learning_rate': 0.00017222222222222224, 'epoch': 5.52}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 700/1000 [2:05:38<53:12, 10.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1768, 'grad_norm': 0.6284176111221313, 'learning_rate': 0.00016666666666666666, 'epoch': 5.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████   | 710/1000 [2:07:24<51:26, 10.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1642, 'grad_norm': 0.7450083494186401, 'learning_rate': 0.0001611111111111111, 'epoch': 5.68}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 720/1000 [2:09:10<49:20, 10.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1762, 'grad_norm': 0.7403860092163086, 'learning_rate': 0.00015555555555555556, 'epoch': 5.76}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 730/1000 [2:10:56<47:33, 10.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1675, 'grad_norm': 0.6119118928909302, 'learning_rate': 0.00015, 'epoch': 5.84}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 740/1000 [2:12:42<46:17, 10.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1869, 'grad_norm': 0.7693789005279541, 'learning_rate': 0.00014444444444444444, 'epoch': 5.92}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▌  | 750/1000 [2:14:29<44:44, 10.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1726, 'grad_norm': 0.7773007750511169, 'learning_rate': 0.0001388888888888889, 'epoch': 6.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 760/1000 [2:16:24<42:53, 10.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0898, 'grad_norm': 0.5452517867088318, 'learning_rate': 0.00013333333333333334, 'epoch': 6.08}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 770/1000 [2:18:10<40:40, 10.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0968, 'grad_norm': 0.6357421875, 'learning_rate': 0.00012777777777777776, 'epoch': 6.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 780/1000 [2:19:56<38:51, 10.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0925, 'grad_norm': 0.5980064868927002, 'learning_rate': 0.00012222222222222221, 'epoch': 6.24}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 79%|███████▉  | 790/1000 [2:21:42<37:21, 10.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0945, 'grad_norm': 0.49177250266075134, 'learning_rate': 0.00011666666666666667, 'epoch': 6.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 800/1000 [2:23:28<35:20, 10.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0902, 'grad_norm': 0.5156549215316772, 'learning_rate': 0.0001111111111111111, 'epoch': 6.4}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 810/1000 [2:25:14<33:33, 10.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0914, 'grad_norm': 0.5580763220787048, 'learning_rate': 0.00010555555555555555, 'epoch': 6.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▏ | 820/1000 [2:27:00<31:55, 10.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0866, 'grad_norm': 0.578541100025177, 'learning_rate': 0.0001, 'epoch': 6.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 830/1000 [2:28:46<30:08, 10.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0903, 'grad_norm': 0.4939974248409271, 'learning_rate': 9.444444444444444e-05, 'epoch': 6.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 840/1000 [2:30:33<28:18, 10.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0917, 'grad_norm': 0.5699628591537476, 'learning_rate': 8.888888888888889e-05, 'epoch': 6.72}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 850/1000 [2:32:18<26:28, 10.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0995, 'grad_norm': 0.6883973479270935, 'learning_rate': 8.333333333333333e-05, 'epoch': 6.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 860/1000 [2:34:04<24:39, 10.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.085, 'grad_norm': 0.4961504340171814, 'learning_rate': 7.777777777777778e-05, 'epoch': 6.88}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 870/1000 [2:35:51<23:02, 10.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0937, 'grad_norm': 0.7600173950195312, 'learning_rate': 7.222222222222222e-05, 'epoch': 6.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 880/1000 [2:37:48<22:57, 11.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.078, 'grad_norm': 0.3396884500980377, 'learning_rate': 6.666666666666667e-05, 'epoch': 7.04}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%|████████▉ | 890/1000 [2:39:37<19:59, 10.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0621, 'grad_norm': 0.3332424759864807, 'learning_rate': 6.111111111111111e-05, 'epoch': 7.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 900/1000 [2:41:25<18:00, 10.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0608, 'grad_norm': 0.6012160778045654, 'learning_rate': 5.555555555555555e-05, 'epoch': 7.2}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 910/1000 [2:43:13<16:15, 10.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0532, 'grad_norm': 0.36193495988845825, 'learning_rate': 5e-05, 'epoch': 7.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 920/1000 [2:45:00<14:15, 10.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0555, 'grad_norm': 0.37057361006736755, 'learning_rate': 4.4444444444444447e-05, 'epoch': 7.36}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 930/1000 [2:46:47<12:23, 10.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.056, 'grad_norm': 0.30558010935783386, 'learning_rate': 3.888888888888889e-05, 'epoch': 7.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|█████████▍| 940/1000 [2:48:32<10:34, 10.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0547, 'grad_norm': 0.3327709436416626, 'learning_rate': 3.3333333333333335e-05, 'epoch': 7.52}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 950/1000 [2:50:19<08:51, 10.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0551, 'grad_norm': 0.33464133739471436, 'learning_rate': 2.7777777777777776e-05, 'epoch': 7.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 960/1000 [2:52:05<07:03, 10.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0614, 'grad_norm': 0.31255850195884705, 'learning_rate': 2.2222222222222223e-05, 'epoch': 7.68}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 970/1000 [2:53:50<05:16, 10.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0572, 'grad_norm': 0.4057309031486511, 'learning_rate': 1.6666666666666667e-05, 'epoch': 7.76}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 980/1000 [2:55:37<03:32, 10.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0508, 'grad_norm': 0.2755782902240753, 'learning_rate': 1.1111111111111112e-05, 'epoch': 7.84}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 990/1000 [2:57:23<01:46, 10.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0593, 'grad_norm': 0.3417036831378937, 'learning_rate': 5.555555555555556e-06, 'epoch': 7.92}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [2:59:09<00:00, 10.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0552, 'grad_norm': 0.3518241047859192, 'learning_rate': 0.0, 'epoch': 8.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [2:59:09<00:00, 10.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 10749.9272, 'train_samples_per_second': 1.488, 'train_steps_per_second': 0.093, 'train_loss': 0.45283611911535265, 'epoch': 8.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=0.45283611911535265, metrics={'train_runtime': 10749.9272, 'train_samples_per_second': 1.488, 'train_steps_per_second': 0.093, 'total_flos': 3.253279111339868e+17, 'train_loss': 0.45283611911535265, 'epoch': 8.0})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SSucUeB4ulB9",
        "outputId": "88f008b5-e68b-4949-d695-4d0de17cdd5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100%|██████████| 33/33 [00:11<00:00,  2.85it/s]\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_19816\\2188625085.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  param.copy_(torch.load(os.path.join(ckpt_dir,name)))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table style=\"border:1px solid black\" >\n",
              "  <tr>\n",
              "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
              "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
              "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">``</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">▶▶ 2019-2020 School Year\n",
              "The 2019-2020 school year is here! We are so excited to welcome our new students and families to the school. We are also excited</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"># -*- coding: utf-8 -*-\n",
              "#\n",
              "# This file is part of Superdesk.\n",
              "#\n",
              "# Copyright 2013, 2014 Sourcefabric z.u. and</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`import`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"> import Foundation\n",
              "\n",
              "public extension NSURL {\n",
              "    public var absoluteString: String {\n",
              "        return String(cString: CFBundleGetBundleWithURL(self).UTF8String)\n",
              "    }\n",
              "}</s><s>package com.google.</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">import os\n",
              "import re\n",
              "import subprocess\n",
              "from setuptools import setup, find_packages, Command\n",
              "try:\n",
              "    # Python 2 backwards compat\n",
              "    from __builtin__ import raw_input as input\n",
              "except ImportError:</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`from`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">from __future__ import absolute_import\n",
              "from __future__ import division\n",
              "from __future__ import print_function\n",
              "\n",
              "import os\n",
              "import sys\n",
              "\n",
              "from absl import flags\n",
              "\n",
              "from tensorflow.python import pywrap</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">from __future__ import absolute_import\n",
              "\n",
              "import random\n",
              "from .exception import FrameError\n",
              "\n",
              "\n",
              "class Frame(object):\n",
              "    \"\"\"\n",
              "    A partial, text only, implementation of the WebSocket protocol.\n",
              "    For more info</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`while`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">while(1)\n",
              "while(1) {\n",
              "    // do something\n",
              "}\n",
              "\\end{code}\n",
              "\n",
              "Comment: This is not the same as the OP's code.\n",
              "\n",
              "Comment: @Jeffrey: It's</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">while (true) {\n",
              "\tint ready;\n",
              "\n",
              "\tready = signal_to_check(n, SIGINT, &r);\n",
              "\n",
              "\tif (ready < 0) {\n",
              "\t\tperror(\"signal_to_</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`try`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">try to find the best solution for your needs.\n",
              "We are a team of professionals with a long experience in the field of web development.\n",
              "We are a team of professionals with a long experience in the field of web development. We are a</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">try:\n",
              "    from .nbody_graph_search import Ugraph\n",
              "except (ImportError, SystemError, ValueError):\n",
              "    # not installed as a package\n",
              "    from nbody_graph_search import Ugraph\n",
              "\n",
              "# This</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`if`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">if ( !window.atmosphere ) {\n",
              "    window.atmosphere = {};\n",
              "}\n",
              "\n",
              "(function () {\n",
              "    var o = atmosphere.util,\n",
              "        atmosphere = atmosphere.atmosphere = function () {\n",
              "\n",
              "</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">if __name__ == '__main__':\n",
              "    import os\n",
              "    import sys\n",
              "    from ipf.ipf_xml_parser import IPFXMLParser\n",
              "    from ipf.ipf_utils import read_file, dump</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`for`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">for the 2019-2020 school year.\n",
              "The application process for the 2019-2020 school year is now open.\n",
              "The application process for the 2019-20</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">for (var i = 0; i < 100; i++) {\n",
              "  var name = \"ABC\" + i;\n",
              "  register(name);\n",
              "}\n",
              "\n",
              "function register(name) {\n",
              "  var person = {</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`torch`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">torchbearer 2017-05-18 19:55:25 UTC #1\n",
              "I’m a newbie to the world of RPGs, and I’m looking for a game that</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">torch.distribute.launcher = function(opt)\n",
              "% Launcher for distributed training\n",
              "%\n",
              "% Inputs:\n",
              "%   opt: config parameter for training, see number of parameters below\n",
              "%\n",
              "% Outputs:\n",
              "%</pre></td>\n",
              "  </tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# This template helps to compare generated code samples in pretty table form\n",
        "# feel free to present your work in other forms\n",
        "import os\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "out_length = 50\n",
        "prompts =  ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch']\n",
        "\n",
        "table_template = \"\"\"<table style=\"border:1px solid black\" >\n",
        "  <tr>\n",
        "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
        "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
        "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
        "  </tr>\n",
        "{}\n",
        "</table>\"\"\"\n",
        "\n",
        "row_template = '''  <tr>\n",
        "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`{}`</pre></td>\n",
        "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
        "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
        "  </tr>'''\n",
        "\n",
        "rows = []\n",
        "\n",
        "noft = dict()\n",
        "ft = dict()\n",
        "\n",
        "# re-load the model to remove any previous PEFT tuners\n",
        "\n",
        "model_name = 'Enoch/llama-7b-hf'\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n",
        "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "model.eval()\n",
        "for prompt in prompts:\n",
        "    \n",
        "    batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "    for i in range(out_length):\n",
        "        next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "        batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "        batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "    noft[prompt] = tokenizer.decode(batch['input_ids'][0,1:].cpu().numpy().tolist())\n",
        "\n",
        "ckpt_dir = \"checkpoint/checkpoint-1000\"\n",
        "lora_rank = 8\n",
        "for name, module in model.model.layers.named_modules():\n",
        "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
        "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
        "        module.mlp.gate_proj = LoRALayer(module.mlp.gate_proj,rank = lora_rank)\n",
        "        module.mlp.down_proj = LoRALayer(module.mlp.down_proj,rank = lora_rank)\n",
        "model.lm_head = LoRALayer(model.lm_head,rank = lora_rank)\n",
        "\n",
        "for name,param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        param.requires_grad_(False)\n",
        "        param.copy_(torch.load(os.path.join(ckpt_dir,name)))\n",
        "\n",
        "\n",
        "\n",
        "for prompt in prompts:\n",
        "    \n",
        "    batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "    for i in range(out_length):\n",
        "        next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "        batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "        batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "    # replace placeholders in the format() arguments\n",
        "    ft[prompt] = tokenizer.decode(batch['input_ids'][0,1:].cpu().numpy().tolist())\n",
        "\n",
        "    rows.append(row_template.format(prompt, noft[prompt], ft[prompt]))\n",
        "\n",
        "display(HTML(table_template.format('\\n'.join(rows))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrKidv5KulB9"
      },
      "source": [
        "If you reach this: congratulations! you've completed everything in this practice session.\n",
        "\n",
        "If you want to dig deeper, try to implement prompt-tuning (for bonus points!).\n",
        "You can read more about prompt tuning variants in paper [1](https://arxiv.org/abs/2104.08691) or paper [2](https://arxiv.org/abs/2101.00190). Both versions can be implemented by passing trainable prompts as `model.forward(..., past_key_values=your_prompts)`.\n",
        "\n",
        "\n",
        "\n",
        "### Read more\n",
        "\n",
        "* How post-training quantization works: https://arxiv.org/abs/2208.07339\n",
        "* An overview of running large models: https://huggingface.co/docs/accelerate/package_reference/big_modeling\n",
        "* A general library for different adapter types: https://adapterhub.ml/\n",
        "\n",
        "\n",
        "### [extra info] Running other models.\n",
        "\n",
        "This notebook's code can run with other models of similar size, such as [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [OPT-6.7B](https://huggingface.co/facebook/opt-6.7b) or [BLOOM-7.1B](https://huggingface.co/bigscience/bloom-7b1). However, they will require minor code tweaks:\n",
        "1. change the model name in `AutoModelForCausalLM.from_pretrained()` __and__ `AutoTokenizer`\n",
        "2. In the prompt tuning code, change `model.model.embed_tokens` to refer to the target model's word embeddings. Simply `print(model)` to navigate to them.\n",
        "3. Change code to add Lora layers - specifically where you what the transformer block components, since those components now have different names."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09b07105e4f54d2bb5e6cc1c1f1a9c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25e1f3d72230485c9b84cae4f685a69a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bd4b6acd8004c1e98c064708108938e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c4ef0dab98410d88891a2c27fdb5c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3321598939fd4d76957155f58097fadd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31c4ef0dab98410d88891a2c27fdb5c1",
            "placeholder": "​",
            "style": "IPY_MODEL_54b23e64bbc444b4abc3f25832e3677d",
            "value": " 32/32 [00:00&lt;00:00, 325.58 examples/s]"
          }
        },
        "3ee1280bc2b6439e8298f0ea8c74d30e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "530d66e4732b4d5486165654415bd2dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54b23e64bbc444b4abc3f25832e3677d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6aad5b046def4a7db1048434e874b5d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7220ba464c234cbba33068f18f68e7c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aea6cd9a18b4dd9b40bbe65fb0b9069": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cc587f710c94a72976f67013c0d18f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a83adb34773a459a89ae687f328b1aa2",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ee1280bc2b6439e8298f0ea8c74d30e",
            "value": 33
          }
        },
        "7db40f2dcb2e46309b29e137eac7bba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93ed39f5849c493eaa8035bd3d11047b",
            "placeholder": "​",
            "style": "IPY_MODEL_845a855f36124f03a30829e21df98702",
            "value": " 33/33 [01:52&lt;00:00,  3.35s/it]"
          }
        },
        "845a855f36124f03a30829e21df98702": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "895c44b30fd24b8b9bbedc631a7934ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ad2b69a25304e5f903f2fd43b538340",
            "placeholder": "​",
            "style": "IPY_MODEL_7aea6cd9a18b4dd9b40bbe65fb0b9069",
            "value": "Map: 100%"
          }
        },
        "8aaa22971b3e416eae8394ef3b3b3f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dba48e929a2e43ec8e4bb3d4b32475ca",
            "placeholder": "​",
            "style": "IPY_MODEL_530d66e4732b4d5486165654415bd2dc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8ad2b69a25304e5f903f2fd43b538340": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "923869a5864c4d3d80fb76c99fff24e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92f3ee2ed68c4defbed2fe9bef0f20b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f72b2d490b04440b800ac3c8ab05e625",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6ddf10ea9ef499f917c81fde3e63cd2",
            "value": 32
          }
        },
        "93ed39f5849c493eaa8035bd3d11047b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94aa4f70041942639c8759a9e371b80e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7adb06901b34e03893f30ecc23b97ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d94f975c69b7421f9851edeff1acbc1d",
              "IPY_MODEL_7cc587f710c94a72976f67013c0d18f1",
              "IPY_MODEL_7db40f2dcb2e46309b29e137eac7bba2"
            ],
            "layout": "IPY_MODEL_b6013ba4a99743b3a00f7a51a366a507"
          }
        },
        "a83adb34773a459a89ae687f328b1aa2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a868f8a190f74df2a99a50e2ca9a7cb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_895c44b30fd24b8b9bbedc631a7934ec",
              "IPY_MODEL_92f3ee2ed68c4defbed2fe9bef0f20b2",
              "IPY_MODEL_3321598939fd4d76957155f58097fadd"
            ],
            "layout": "IPY_MODEL_f0fe9da3411840ef8d7eff80883cb8e9"
          }
        },
        "b6013ba4a99743b3a00f7a51a366a507": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b87c54c3bed847ab933eae8175359169": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8aaa22971b3e416eae8394ef3b3b3f0f",
              "IPY_MODEL_e9c4ba0d262c4b76baa00532e209b92f",
              "IPY_MODEL_e6f9064e6ec545debe2e90163f4c712c"
            ],
            "layout": "IPY_MODEL_6aad5b046def4a7db1048434e874b5d5"
          }
        },
        "c6ddf10ea9ef499f917c81fde3e63cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d94f975c69b7421f9851edeff1acbc1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7220ba464c234cbba33068f18f68e7c8",
            "placeholder": "​",
            "style": "IPY_MODEL_94aa4f70041942639c8759a9e371b80e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "dba48e929a2e43ec8e4bb3d4b32475ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6f9064e6ec545debe2e90163f4c712c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_923869a5864c4d3d80fb76c99fff24e2",
            "placeholder": "​",
            "style": "IPY_MODEL_25e1f3d72230485c9b84cae4f685a69a",
            "value": " 33/33 [01:49&lt;00:00,  3.12s/it]"
          }
        },
        "e9c4ba0d262c4b76baa00532e209b92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bd4b6acd8004c1e98c064708108938e",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09b07105e4f54d2bb5e6cc1c1f1a9c8e",
            "value": 33
          }
        },
        "f0fe9da3411840ef8d7eff80883cb8e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f72b2d490b04440b800ac3c8ab05e625": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
