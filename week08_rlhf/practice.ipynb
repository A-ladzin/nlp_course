{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvhPa7a59AIG"
      },
      "source": [
        "<font color=red>**Danger zone:**</font> you'll be fine-tuning a model to generate positive, negative or even toxic reviews. We'll be doing this for fun, but this is also the technique for [review bombing](https://en.wikipedia.org/wiki/Review_bomb), bot farms on social media and other less than dignified stuff. It is ultimately your decision how you apply this knowledge, but before you choose, ask yourself: is this why you chose to learn ML?\n",
        "\n",
        "\n",
        "# LLMs Alignment with Reinforcement Learning from human feedback (RLHF).\n",
        "\n",
        "_based on the [original notebook](https://github.com/antndlcrx/oxford-llms-workshop/blob/main/materials/seminars/day_3/8_LLMs%20alignment%20with%20RLHF.ipynb) by Ilya Boytsov for the Oxford LLMs workshop_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgfL4bSSAXan"
      },
      "source": [
        "In this session, you're gonna fine-tune a language model with reinforcement learning to make it generate good (or bad) reviews.\n",
        "\n",
        "To perform RL-based fine-tuning, we'll use a new (in this course) library called [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl). TRL implements the main reinforcement learning components of RLHF: reward modeling and fine-tuning with PPO.\n",
        "\n",
        "![img](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/TRL-readme.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python -m pip install --upgrade --ignore-installed setuptools accelerate trl==0.7.4 transformers==4.33.1 datasets==2.14.4 peft==0.5.0 tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cJfrTbFYAx8"
      },
      "source": [
        "### Tutorial: align the model to generate positive movie reviews\n",
        "\n",
        "To see how TRL works, we'll use it to align GPT2 on IMDB dataset to generate positive (or negative) movie reviews. In fact, __it's your choice whether you want positive or negative reviews.__\n",
        "\n",
        "But before you choose, let's take a look at the baseline model: a GPT-2 fine-tuned on generating arbitrary movie reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pHs22MXdPify"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "main_tokenizer = transformers.AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
        "main_model = transformers.AutoModelForCausalLM.from_pretrained(\"lvwerra/gpt2-imdb\", device_map=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KE3jo7uhQrvK",
        "outputId": "6ae43c17-7ecc-4db7-c7c8-1e4975c621b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated text: The movie was really bad. I found the acting to be average, the storyline to be lame, and the plot to be uninteresting (uncompelling). The acting wasn't great and the film was about as good as Hell. Also this was the worst\n"
          ]
        }
      ],
      "source": [
        "inputs = main_tokenizer(\"The movie\", return_tensors='pt').to(device)\n",
        "generated_ids = main_model.generate(**inputs, max_new_tokens=50, do_sample=True)\n",
        "print(\"\\nGenerated text:\", main_tokenizer.decode(generated_ids.flatten().cpu().numpy().tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJbfhMEpR4Sz"
      },
      "source": [
        "If you run this cell a couple of times, you'll see that the model generates both positive, negative and neutral reviews in some proportion. What we're gonna do next is teach the model to generate more positive (or negative) reviews.\n",
        "\n",
        "Similarly to InstructGPT, we're gonna do that in 2 stages:\n",
        "- **train a reward model** to assign higher values to positive (or negative) reviews\n",
        "- fine-tune the language model to **maximize that reward using [proximal policy optimization](https://openai.com/research/openai-baselines-ppo)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcv4uC7xb26Z"
      },
      "source": [
        "## Stage 1: train a reward model\n",
        "\n",
        "First, we'll train a BERT-like model as our reward model. We'll generate a synthetic pairwise rankings to emulate human rankings.\n",
        "\n",
        "__Q:__ why do I need a reward model? Can I just use a pre-trained sentiment classifier? <br> __A:__ Yes, you can - but that only works for movie reviews. But this tutorial will teach you how to do RLHF for any kind objective.\n",
        "\n",
        "\n",
        "__If you actually want to maximize sentiment (or other \"label\") instead of human preferences, train reward model as a classifier! (see week5)__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WeOdZ_ayc9dy",
        "outputId": "0dd54557-4237-4a30-d1b9-0ca00d7a7d04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# We'll be fine-tuning a small BERT-like model for now. Please try other models for the main assignment.\n",
        "reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", device_map=device)\n",
        "reward_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZUUNQo-d11b"
      },
      "source": [
        "__Note that__ the reward model has a separate tokenizer, different from the main model. They don't need to be the same for RLHF fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "TTWR-48ZXQX6"
      },
      "outputs": [],
      "source": [
        "# To train a reward model, you need a dataset (or generator) of positive-negative pairs.\n",
        "# Each training sample should be a dict with 4 keys:\n",
        "#  - input_ids_chosen, attention_mask_chosen = tokenizer(\"A sentence that human labeler likes more\")\n",
        "#  - input_ids_rejected, attention_mask_rejected = tokenizer(\"A sentence that human labeler likes less\")\n",
        "\n",
        "import torch\n",
        "import datasets\n",
        "\n",
        "class IMDBPairwiseDataset(torch.utils.data.Dataset):\n",
        "    \"\"\" A dataset of all possible pairs of chosen and texts in TRT reward training format \"\"\"\n",
        "    def __init__(self, imdb, tokenizer, accepted_label: int):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.chosen_texts = [row['text'] for row in imdb if row['label'] == accepted_label]\n",
        "        self.rejected_texts = [row['text'] for row in imdb if row['label'] != accepted_label]\n",
        "        assert self.chosen_texts, f\"no texts with label {accepted_label}\"\n",
        "        print(f\"Found {len(self.chosen_texts)} chosen and {len(self.rejected_texts)} rejected texts, {len(self)} pairs\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.chosen_texts) * len(self.rejected_texts)  # all pairs\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        chosen = self.tokenizer(self.chosen_texts[index // len(self.chosen_texts)], truncation=True)\n",
        "        rejected = self.tokenizer(self.rejected_texts[index % len(self.chosen_texts)], truncation=True)\n",
        "        return dict(input_ids_chosen=chosen['input_ids'], attention_mask_chosen=chosen['attention_mask'],\n",
        "                    input_ids_rejected=rejected['input_ids'], attention_mask_rejected=rejected['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "olo-bvgNcwEC",
        "outputId": "16051d61-c450-4a3e-8689-f91f28f8280e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 12500 chosen and 12500 rejected texts, 156250000 pairs\n",
            "CHOSEN: [CLS] If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story. < br / > < br / > One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives ( unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film ). < br / > < br / > One might better spend one's time staring out a window at a tree growing. < br / > < br / > [SEP]\n",
            "REJECTED: [CLS] This movie has some things that are pretty amazing. First, it is supposed to be based on a true story. That, in itself, is amazing that multiple tornadoes would hit the same town at night in the fall - in Nebraska. I wonder if the real town's name was close to \" Blainsworth \" ( which is the town's name in the movie ). There is an Ainsworth, Nebraska, but there is also a town that starts with Blains - something. < br / > < br / > It does show the slowest moving tornadoes on record in the the seen where the boys are in the house. On the other hand, the scene where the TV goes fuzzy is based in fact. Before Doppler radar and weather radio, we were taught that if you turned your TV to a particular channel ( not on cable ) and tuned the brightness just right, you could tell if there was a tornado coming. The problem was that by then you would be able to hear it. < br / > < br / > Since I know something about midwest tornadoes, it made this movie fun for me. I enjoy it more than Twister. I mean, give me a break - there is no way you could make it through and F5 by chaining yourself to a pipe in a well house. [SEP]\n"
          ]
        }
      ],
      "source": [
        "TARGET_LABEL = 0   # and make sure it works by reviewing the sample printed below\n",
        "imdb = datasets.load_dataset(\"imdb\", split='train')\n",
        "reward_data = IMDBPairwiseDataset(imdb, reward_tokenizer, accepted_label=TARGET_LABEL)\n",
        "\n",
        "sample = reward_data[31337]\n",
        "print('CHOSEN:', reward_tokenizer.decode(sample['input_ids_chosen']))\n",
        "print('REJECTED:', reward_tokenizer.decode(sample['input_ids_rejected']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZRczyofiSl0"
      },
      "source": [
        "We'll be using `trl.RewardTrainer` - a special case of `transformers.Trainer` that you used in the past. `RewardTrainer` accepts the same format of training arguments (e.g. batch size, gradient checkpointing) as before, except that it trains the model for the pairwise reward objective from [the InstructGPT paper](https://arxiv.org/pdf/2203.02155.pdf):\n",
        "\n",
        "![img](https://i.imgur.com/2JzNAPs.png)\n",
        "\n",
        "Note that the model itself does not score pairs: it processes chosen ($y_w$) and rejected ($y_l$) samples independently. To minimize this loss, the reward model needs to score chosen sample higher than the rejected one. Note that the formula also assumes some context $x$, which is useful for seq2seq tasks. In our case of movie reviews, $x$ is empty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1053
        },
        "id": "oaQ_-JAzakJs",
        "outputId": "4ffe023f-4773-4a47-8af2-86839db874b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\reward_trainer.py:182: UserWarning: When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig. It will be set to `512` by default, but you should do it yourself in the future.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\reward_trainer.py:199: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "  5%|▌         | 50/1000 [00:45<11:25,  1.39it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5091, 'grad_norm': 3.0127673149108887, 'learning_rate': 1.34091e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 100/1000 [01:21<10:47,  1.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1947, 'grad_norm': 2.138087749481201, 'learning_rate': 1.2718200000000001e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 150/1000 [01:57<10:10,  1.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1377, 'grad_norm': 2.3117825984954834, 'learning_rate': 1.20132e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 200/1000 [02:33<09:31,  1.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1258, 'grad_norm': 3.006094217300415, 'learning_rate': 1.1308200000000001e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 250/1000 [03:09<08:58,  1.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1, 'grad_norm': 7.099982261657715, 'learning_rate': 1.06032e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 300/1000 [03:45<08:18,  1.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1119, 'grad_norm': 10.28709888458252, 'learning_rate': 9.8982e-06, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 350/1000 [04:20<07:44,  1.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1043, 'grad_norm': 2.178290605545044, 'learning_rate': 9.1932e-06, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 400/1000 [04:56<07:06,  1.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.09, 'grad_norm': 5.269755840301514, 'learning_rate': 8.4882e-06, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 450/1000 [05:32<06:32,  1.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0819, 'grad_norm': 4.6626458168029785, 'learning_rate': 7.7832e-06, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 500/1000 [06:08<05:54,  1.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0823, 'grad_norm': 3.085207223892212, 'learning_rate': 7.0782e-06, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
            " 55%|█████▌    | 550/1000 [06:44<05:22,  1.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0874, 'grad_norm': 2.4558117389678955, 'learning_rate': 6.3732e-06, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 600/1000 [07:21<04:42,  1.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0737, 'grad_norm': 0.978861391544342, 'learning_rate': 5.6682e-06, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 650/1000 [07:57<04:09,  1.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0685, 'grad_norm': 5.672646999359131, 'learning_rate': 4.9632e-06, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 700/1000 [08:32<03:33,  1.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0751, 'grad_norm': 4.7364044189453125, 'learning_rate': 4.2582e-06, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▌  | 750/1000 [09:08<03:03,  1.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.07, 'grad_norm': 9.540241241455078, 'learning_rate': 3.5532e-06, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 800/1000 [09:45<02:21,  1.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0454, 'grad_norm': 0.6404498219490051, 'learning_rate': 2.8482e-06, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 850/1000 [10:20<01:46,  1.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0593, 'grad_norm': 1.604547142982483, 'learning_rate': 2.1432e-06, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 900/1000 [10:56<01:10,  1.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.067, 'grad_norm': 6.464433670043945, 'learning_rate': 1.4382e-06, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 950/1000 [11:31<00:35,  1.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0626, 'grad_norm': 0.36007577180862427, 'learning_rate': 7.332e-07, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [12:07<00:00,  1.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.044, 'grad_norm': 1.3303836584091187, 'learning_rate': 2.82e-08, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [12:09<00:00,  1.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 729.3085, 'train_samples_per_second': 43.877, 'train_steps_per_second': 1.371, 'train_loss': 0.10954286599159241, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=0.10954286599159241, metrics={'train_runtime': 729.3085, 'train_samples_per_second': 43.877, 'train_steps_per_second': 1.371, 'total_flos': 0.0, 'train_loss': 0.10954286599159241, 'epoch': 0.00020479997902848215})"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import trl\n",
        "\n",
        "training_args = trl.RewardConfig( # like transformers.TrainingArguments\n",
        "    output_dir=\"reward_model\",\n",
        "    per_device_train_batch_size=32,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1.41e-5,\n",
        "    max_steps=1_000,              # note: training may need more than 1k steps\n",
        "    logging_steps=50,\n",
        "    gradient_checkpointing=True,  # reduce memory usage but train ~30% slower\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    fp16=True                     # disable this on CPU or on very old GPUs\n",
        "    # you may add any other hyperparameters that you found useful in weeks 5-7\n",
        ")\n",
        "\n",
        "trainer = trl.RewardTrainer(\n",
        "    model=reward_model,\n",
        "    args=training_args,\n",
        "    tokenizer=reward_tokenizer,\n",
        "    train_dataset=reward_data,\n",
        "    peft_config=None,  # optionally, you may tune with LoRA, prompt-tuning, etc\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CRk7z-2r4C-A",
        "outputId": "5b99e451-e2e7-44bb-cf11-c28a8eb8ae64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reward_model.gradient_checkpointing_disable()\n",
        "reward_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZIaS-gRo8yc"
      },
      "source": [
        "### Sanity-check the reward model (1 point)\n",
        "\n",
        "Let's check how our reward model performs.\n",
        "\n",
        "__Your task__ is to measure how often does your reward model can rank a pair of (chosen and rejected) reviews correctly. Please measure this separately for train data (`imdb`) and a separate test set loaded below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IeQ108nOZ7nO",
        "outputId": "6de29424-1308-4677-e909-4f3f9d8ceb6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEXT: This movie sucked. It really was a waste of my life. The acting was atrocious, the plot completely implausible. Long, long story short, these people get \"terrorized\" by this pathetic \"crazed killer\", but completely fail to fight back in any manner. And this is after they take a raft on a camping trip, with no gear, and show up at a campsite that is already assembled and completely stocked with food and clothes and the daughters headphones. Additionally, after their boat goes missing, they panic that they're stuck in the woods, but then the daughters boyfriend just shows up and they apparently never consider that they could just hike out of the woods like he did to get to them. Like I said, this movie sucks. A complete joke. Don't let your girlfriend talk you into watching it.\n",
            "REWARD: 5.3046875\n",
            "LABEL: 0\n",
            "\n",
            "TEXT: Good: Engaging cinematic firefights, great presentation, vehicles are actually fun to drive, fairly appealing multiplayer, faithful to the movie, and the list goes on.<br /><br />Bad: Main missions are a bit short.<br /><br />This game defines what a \"good\" third person shooter(not necessarily a spy-game) is. Great firefights carry on the story and make you want to complete EVERY single mission through, and unlock all the genuine bonuses the game has to offer. The hype this game had, was lived up to, and I personally think you should buy it, and hook up with a couple of friends and play this one. Loads of fun. <br /><br />The sound in this game, is a rip-roaring achievement from a few previous bond games, and firing a weapon, really feels like you're firing a weapon. It ties in with the aspect that you are a deadly and ruthless spy.<br /><br />All in all, this game makes you excited and satisfied after you make it through, and some multiplayer that can compete with the standards of the crafty James Bond \"Nightfire\" game for gamecube.\n",
            "REWARD: -4.7734375\n",
            "LABEL: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for sample_index in 45, 16000:\n",
        "  print('TEXT:', imdb[sample_index]['text'])\n",
        "  inputs = reward_tokenizer(\n",
        "      imdb[sample_index]['text'], truncation=True, return_tensors='pt').to(device)\n",
        "  with torch.no_grad():\n",
        "    reward = reward_model(**inputs).logits[0, 0].item()\n",
        "    print(\"REWARD:\", reward)\n",
        "  print('LABEL:', imdb[sample_index]['label'])\n",
        "  print()\n",
        "\n",
        "# note: your reward model may produce different absolute rewards.\n",
        "# This is fine as long as the rewards are ordered correctly (most of the time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aEevUrfqavnb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 25000/25000 [02:10<00:00, 191.11it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0277544896"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "imdb_test = datasets.load_dataset(\"imdb\", split='test')\n",
        "\n",
        "rewards = []\n",
        "labels = []\n",
        "\n",
        "for idx in tqdm(range(len(imdb_test))):\n",
        "    inputs = reward_tokenizer(imdb_test[idx]['text'], truncation=True, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        reward = reward_model(**inputs).logits[0, 0].item()\n",
        "        rewards.append(reward)\n",
        "        labels.append(imdb_test[idx]['label'])\n",
        "\n",
        "score = roc_auc_score(labels,rewards)\n",
        "roc_auc_score(labels,rewards)\n",
        "\n",
        "\n",
        "# <a whole lot of your code here, feel free to spit it as you see fit>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9722455104"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1-score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHCWHMyRw2-k"
      },
      "source": [
        "### Reward-guided generation (1 point)\n",
        "\n",
        "If you did everything right, by now you should have a decent reward model. Before we use it for reinforcement learning, let's see if we can align model samples without any training.\n",
        "\n",
        "To do so, you can use reward-guided inference: __generate N=16 samples, then select the one with the highest reward__ (according to your reward model).\n",
        "\n",
        "For this problem, it's on you to demonstrate whether or not your code works. Find at least 5 neutral prompts such as \"This movie is\" (...), generate samples, rank them based on reward and show which samples get the highest reward.\n",
        "\n",
        "Note: it is faster to generate samples in parallel, rather than sequentially, as follows:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8BRsyb2cq5dR",
        "outputId": "4d4c917c-4a79-4db8-fff4-a63167256044"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample: It was always my dream to play poker, I always thought I could do it, so I got involved. I think it was interesting to see how popular poker was before getting into poker. Many poker players have been talking about the fact that their poker careers are\n",
            "Sample: It was funny how quickly he was getting old, so his father gave him out from the hospital and he was okay. The doctor thought he died and he went to the same hospital a couple days later that same day. My daughter did not know anything about this\n",
            "Sample: It was interesting how she managed to remain credible throughout; her most memorable scenes were the flashbacks where she visits school and falls for an older older child. We have the young girl, who tries to find a place for her in life but her parents turn her into\n",
            "Sample: It was the first episode where the actors decided to throw in a couple of cheap shots. There is no way there would be another show. There would be worse.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Sample: It was an unusual way for this show to end. In this case the episode ended in a very satisfying climax which is the exact plot that we expect. So in this case we have a plot that ends with a big bang. In this case we just have\n"
          ]
        }
      ],
      "source": [
        "inputs = main_tokenizer([\"It was\"] * 5, return_tensors='pt').to(device)\n",
        "for candidate in main_model.generate(**inputs, max_new_tokens=50, do_sample=True):\n",
        "  print(\"Sample:\", main_tokenizer.decode(candidate.flatten().cpu().numpy().tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(\"reward_model/checkpoint-1000\", device_map=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "r08F4lz7yxE1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "# <YOUR CODE HERE> - feel free to organize it as you see fit\n",
        "outputs = []\n",
        "\n",
        "inputs = main_tokenizer([\"This movie is\"] * 16, return_tensors='pt').to(device)\n",
        "\n",
        "for gen in main_model.generate(**inputs, max_new_tokens=50, do_sample=True):\n",
        "    outputs.append(main_tokenizer.decode(gen.flatten().cpu().numpy().tolist()))\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards = []\n",
        "for i in outputs:\n",
        "    inputs = reward_tokenizer(i, truncation=True, return_tensors='pt').to(device)\n",
        "    rewards.append(reward_model(**inputs).logits[0,0].detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([\"This movie is about a girl who loses her virginity accidentally and her sister gets caught up in it. A story I haven't had to think of any more!! A nice twist to this story!! Also, a really nice action/suspense in the last hour with\",\n",
              "       \"This movie is definitely worth watching. I'd give it a 9, as you get a nice feel for the characters even if you're not the type to read all about them.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\",\n",
              "       'This movie is about two teenagers trying to solve one of the best murder mysteries ever set in Chicago. The leads play \"ghosts\" from one mystery to the next and the mystery is far removed from what they had done in the first film. A mystery for all',\n",
              "       'This movie is like a horror movie, as a bad thing, but you do get a really good one that is not even bad, but is really, really good. It does feel like it is being done about 30 minutes in. It takes it very seriously and',\n",
              "       'This movie is not particularly well known. It is still a film I watched once a year because I hate seeing my own movies being forgotten. One such one is \"The Last Knight\", which was actually shot in 2001 and it is still considered my favorite movie ever.',\n",
              "       'This movie is in many ways like the original \"Narcotus\" (1981 version). A film about the human condition, life and human morality that was also an attempt to be humane in a moralistic age. The original \"Narcotus\" contained',\n",
              "       'This movie is definitely worth watching even if the script is boring and the acting is awful.<br /><br />The storyline of the movie revolves around three girls that live with their parents. These kids are all really good actors who get into acting, and you know',\n",
              "       \"This movie is supposed to give people stories from the past. It is about the old days of an old warlord making an escape. He is captured by an army of thugs. They are the good ol' old guys and his friend is a young man. The\",\n",
              "       'This movie is one of the dumbest I have ever seen and yet one that still keeps on giving me laughs. Not to mention the fact that it is based on a book by William Shakespeare, which I read about a few years back and was the best part of',\n",
              "       'This movie is basically a parody of \"Mad Max\". I thought, in order to keep up with what I like about this movie, the movie makers had to try hard and make it better. At any rate, even though I\\'d expect a lot out of this',\n",
              "       'This movie is definitely NOT for the younger crowd. I watched it once on a big screen. I don\\'t understand the film, but I still think it\\'s not the worst movie of all time. They did a good job. But it\\'s no \"Gone',\n",
              "       'This movie is a little confusing. First of all, this film focuses on a girl (Namby) who is forced into an abusive relationship. Second, this girl is not happy for an abusive boyfriend/husband after she married him or their mother (she does',\n",
              "       'This movie is about a man who tries to solve his life-threatening death. He meets a very attractive woman whom he tries to love. With his dying friend, the movie begins of a good old fashioned mystery about the murders. The story does not make any sense',\n",
              "       \"This movie is not what it seems, because the acting is poor, and the direction is very inconsistent. It doesn't have many action sequences, either, that are good in the short time it lasts. I would not recommend this movie to anybody who wants to see\",\n",
              "       'This movie is filled with all of the trappings of a bad sitcom. It\\'s all the same old cliché: a girl is going around playing golf, is constantly telling her boyfriend to \"go back to the club\" or \"get a haircut.\" It\\'s completely',\n",
              "       'This movie is full of bad acting, bad photography, and bad makeup. Here are the effects, which at times resemble a miniature theater:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'],\n",
              "      dtype='<U458')"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.array(outputs)[np.argsort(rewards)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NjQ40BRoH5f"
      },
      "source": [
        "# Stage 2: fine-tune the main model with RL\n",
        "\n",
        "\n",
        "For this tutorial, we will optimize GPT2 to produce positive IMDB movie reviews using the reward model you trained above.\n",
        "\n",
        "Unlike supervised fine-tuning, RL allows model to generate it's own sentences on each training step. Then, it calculates the reward of those specific sentences, and finally, updates the model to increase the probability of sentences with high reward.\n",
        "\n",
        "Thus, each RLHF consists of three stages: __Rollout__, __Evaluation__ and __Update__\n",
        "\n",
        "<div style=\"text-align: center\">\n",
        "<img src='https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/gpt2_bert_training.png' width='600'>\n",
        "\n",
        "The update stage depends on the specific RL algorithm. We'll be using Proximal Policy Optimization, or [PPO](https://arxiv.org/abs/1707.06347), similarly to what was used for InstructGPT.\n",
        "\n",
        "Before we run those 3 stages, however, we need to create a dataset of \"queries\" - partial reviews in our case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "f6ba50eafdeb4b94a1e7c22c5027f709",
            "63e252cfd9f44ef79137473b618828dd",
            "4ad4da252cb64e818d35eb3869adc81c",
            "d3a8dcfad53b4de885b39ee83e6029ef",
            "44f3d7c434354a90bfa3d4750048b80f",
            "2ca9e640d5d549658e42fd73af8ea399",
            "c8ea1c2d3b5040378d0f2bd213933603",
            "1692cc1532df4c2695c729a964a31da8",
            "55c624445ca44090a2f109d3bf5f3f6a",
            "411cd47238a94edc8283e178e04d386f",
            "9604bff7c9414071a55d063fdf4174fa"
          ]
        },
        "id": "jm5IUrer0xd_",
        "outputId": "ea58969e-cb80-4ff5-8a34-7c6a2d14cab3"
      },
      "outputs": [],
      "source": [
        "import trl\n",
        "# Note: this code is specific to IMDB; you will need to re-write it for other tasks\n",
        "imdb_for_rlhf = imdb.filter(lambda row: len(row['text']) > 200, batched=False)\n",
        "imdb_for_rlhf = imdb_for_rlhf.remove_columns(['label'])\n",
        "sample_length = trl.core.LengthSampler(2, 8)  # use the first 2-8 tokens as query\n",
        "\n",
        "def select_query_and_tokenize(sample):\n",
        "    query_ids = main_tokenizer.encode(sample[\"text\"])[: sample_length()]\n",
        "    sample[\"query\"] = main_tokenizer.decode(query_ids)  # query is the only required column\n",
        "    sample[\"input_ids\"] = query_ids  # to avoid re-tokenizing later\n",
        "    return sample  # we do not need the rest - it will be generated by the model\n",
        "\n",
        "imdb_for_rlhf = imdb_for_rlhf.map(select_query_and_tokenize, batched=False)\n",
        "imdb_for_rlhf.set_format(type=\"torch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKIAyilP3Bf1"
      },
      "source": [
        "Next, let's prepare your reward model to predict rewards on whatever reviews were generated. Note that we use plaintext reviews because main model uses a different tokenizer from the reward model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kkm4MLOr20Jk"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "def compute_reward(texts: List[str]) -> torch.Tensor:\n",
        "  inputs = reward_tokenizer(texts, truncation=True, padding=True, return_tensors='pt').to(device)\n",
        "  with torch.no_grad():\n",
        "    return reward_model(**inputs).logits[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7wJto13M3vWu",
        "outputId": "0214bd72-21e0-49ea-ec33-12f9e9d587d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 5.3037, -4.7717], device='cuda:0')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_reward([imdb[45]['text'], imdb[16000]['text']])  # test on human-written reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3buACYV4QLJ"
      },
      "source": [
        "Finally, we move to RL training. In this tutorial, we'll train LoRA adapters and not the full model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nar1yXgl4KQa",
        "outputId": "0dfa2e71-48ef-497b-90cf-e156fa921c7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,179,648 || all params: 125,620,225 || trainable%: 0.9391\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import peft\n",
        "peft_config = peft.LoraConfig(\n",
        "    task_type=peft.TaskType.CAUSAL_LM, r=32, lora_alpha=32, lora_dropout=0.0, inference_mode=False\n",
        ")\n",
        "\n",
        "# reload main model as AutoModelForCausalLMWithValueHead - with an extra head needed for PPO\n",
        "main_tokenizer = transformers.AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
        "main_tokenizer.pad_token = main_tokenizer.eos_token\n",
        "\n",
        "main_model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(\"lvwerra/gpt2-imdb\", device_map=device)\n",
        "main_model = peft.get_peft_model(main_model, peft_config, adapter_name='default')\n",
        "main_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIQK5bcpCPZ6"
      },
      "source": [
        "Same as before, trl has a special type of trainer that minimize PPO-specific pseudo-loss. You can read more on this trainer [here](https://huggingface.co/docs/trl/main/en/ppo_trainer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EvTtiLs94txE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
            "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
            "the same time. Both libraries are known to be incompatible and this\n",
            "can cause random crashes or deadlocks on Linux when loaded in the\n",
            "same Python program.\n",
            "Using threadpoolctl may cause crashes or deadlocks. For more\n",
            "information and possible workarounds, please see\n",
            "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
            "\n",
            "  warnings.warn(msg, RuntimeWarning)\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_args = trl.PPOConfig(\n",
        "    model_name=main_model.config._name_or_path,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1.41e-5,\n",
        "    batch_size=64,\n",
        "    mini_batch_size=16,\n",
        "    ppo_epochs=4,                 # PPO performs this many updates per training batch\n",
        ")\n",
        "\n",
        "ppo_trainer = trl.PPOTrainer(\n",
        "    training_args, model=main_model.model, tokenizer=main_tokenizer,\n",
        "    dataset=imdb_for_rlhf, data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0])\n",
        ")  # note: we pass main_model.model because PPOTrainer checks for one of several supported model types ...\n",
        "# ... main_model.model is a model with adapters, which is supported. main_model itself is a wrapper that is not supported"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "74b0875944e047e6a4d09cc988013ae8",
            "bc859c36e95646b78e9a08111ce1735b",
            "98156b41fab9493cac7eb8edfd1f611a",
            "f0adf0ab77d74ff3857ffa5b9b1a0373",
            "6d3c3f5bfad345f68b6e62ab870e69bf",
            "5cdc9539cca3499abb4958d40142d77c",
            "a984b403d0464e88b4539d586dfc4cc2",
            "23db3f8d31cc4c5d98af465767af45af",
            "4922d11311b846f59278623ec5b6dd39",
            "c487b0154d434955ba8070253af01e1c",
            "c270953012ea437fa8ff299292a41837"
          ]
        },
        "id": "eYr-w666-QfK",
        "outputId": "96206a86-ee25-4e74-a950-f52be855cc24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "  2%|▏         | 1/50 [00:22<18:12, 22.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 0 ------------------------------\n",
            "rewards/mean:\t0.537303925\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.027607590\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.000000000\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 2/50 [00:43<17:33, 21.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 1 ------------------------------\n",
            "rewards/mean:\t-0.114208698\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t-0.073779672\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.000067968\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 3/50 [01:05<16:57, 21.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 2 ------------------------------\n",
            "rewards/mean:\t0.485072792\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.053837083\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.008635592\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 4/50 [01:28<17:05, 22.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 3 ------------------------------\n",
            "rewards/mean:\t0.098857373\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t-0.034162991\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.008405432\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 5/50 [01:48<16:03, 21.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 4 ------------------------------\n",
            "rewards/mean:\t0.669786990\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.031738099\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.013997542\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 6/50 [02:12<16:21, 22.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 5 ------------------------------\n",
            "rewards/mean:\t0.363579839\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.007205085\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.027143134\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 7/50 [02:34<15:59, 22.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 6 ------------------------------\n",
            "rewards/mean:\t0.579912066\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.013163842\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.017383823\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 8/50 [02:55<15:19, 21.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 7 ------------------------------\n",
            "rewards/mean:\t0.674233437\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.067161046\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.041535616\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 9/50 [03:19<15:19, 22.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 8 ------------------------------\n",
            "rewards/mean:\t-0.135698825\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t-0.104912646\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.035695463\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 10/50 [03:40<14:40, 22.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 9 ------------------------------\n",
            "rewards/mean:\t0.322107166\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t-0.008312296\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.025565404\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 11/50 [04:02<14:17, 22.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 10 ------------------------------\n",
            "rewards/mean:\t0.320143789\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t-0.028853713\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.073386572\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 12/50 [04:25<14:02, 22.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 11 ------------------------------\n",
            "rewards/mean:\t0.422156900\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.053277723\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.045664683\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 13/50 [04:46<13:30, 21.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 12 ------------------------------\n",
            "rewards/mean:\t0.416449904\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.020355772\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.127303272\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 14/50 [05:08<13:07, 21.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 13 ------------------------------\n",
            "rewards/mean:\t0.380871922\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.036533356\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.134921893\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 15/50 [05:30<12:46, 21.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 14 ------------------------------\n",
            "rewards/mean:\t0.888844609\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.072935693\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.122885332\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 16/50 [05:50<12:13, 21.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 15 ------------------------------\n",
            "rewards/mean:\t-0.019307733\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t-0.083748966\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.149929971\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 17/50 [06:14<12:09, 22.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 16 ------------------------------\n",
            "rewards/mean:\t0.821264982\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.129028842\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.088150516\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 18/50 [06:35<11:36, 21.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 17 ------------------------------\n",
            "rewards/mean:\t1.070811272\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.116673350\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.105175868\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 19/50 [06:57<11:15, 21.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 18 ------------------------------\n",
            "rewards/mean:\t0.234880865\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t-0.015775513\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.102224238\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 20/50 [07:20<11:07, 22.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 19 ------------------------------\n",
            "rewards/mean:\t0.551761866\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.046502981\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.202081412\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 21/50 [07:41<10:31, 21.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 20 ------------------------------\n",
            "rewards/mean:\t0.866803169\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.120542631\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.207578003\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 22/50 [08:03<10:15, 21.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 21 ------------------------------\n",
            "rewards/mean:\t0.540445507\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.069162190\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.124114834\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 23/50 [08:25<09:56, 22.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 22 ------------------------------\n",
            "rewards/mean:\t0.262418449\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.001951750\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.188866973\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 24/50 [08:46<09:23, 21.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 23 ------------------------------\n",
            "rewards/mean:\t-0.241178378\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t-0.072451100\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.227816850\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 25/50 [09:08<09:03, 21.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 24 ------------------------------\n",
            "rewards/mean:\t0.091523319\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t-0.043856263\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.067279354\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 26/50 [09:30<08:46, 21.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 25 ------------------------------\n",
            "rewards/mean:\t-0.265032053\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t-0.104704857\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.313671231\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|█████▍    | 27/50 [09:52<08:23, 21.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 26 ------------------------------\n",
            "rewards/mean:\t0.601075888\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.055311732\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.252364814\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 28/50 [10:14<08:03, 21.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 27 ------------------------------\n",
            "rewards/mean:\t0.441175282\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.045060780\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.223596737\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 29/50 [10:35<07:34, 21.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 28 ------------------------------\n",
            "rewards/mean:\t0.336502433\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.004063313\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.274217635\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 30/50 [10:57<07:12, 21.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 29 ------------------------------\n",
            "rewards/mean:\t0.585320950\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.034546711\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.356056690\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 31/50 [11:20<07:00, 22.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 30 ------------------------------\n",
            "rewards/mean:\t0.866914034\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.087734923\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.352633238\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 32/50 [11:40<06:27, 21.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 31 ------------------------------\n",
            "rewards/mean:\t-0.121526994\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t-0.036884651\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.340081692\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|██████▌   | 33/50 [12:02<06:09, 21.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 32 ------------------------------\n",
            "rewards/mean:\t0.681454182\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.098433793\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.466585100\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 34/50 [12:25<05:53, 22.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 33 ------------------------------\n",
            "rewards/mean:\t0.118767433\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t-0.013490248\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.312696993\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 35/50 [12:47<05:27, 21.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 34 ------------------------------\n",
            "rewards/mean:\t0.918653429\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.150502622\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.534421563\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 36/50 [13:09<05:07, 21.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 35 ------------------------------\n",
            "rewards/mean:\t0.457341045\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.042851694\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.288924873\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 37/50 [13:31<04:46, 22.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 36 ------------------------------\n",
            "rewards/mean:\t0.555555880\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.097869538\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.370789826\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 38/50 [13:52<04:21, 21.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 37 ------------------------------\n",
            "rewards/mean:\t0.347857088\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.055577446\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.571614385\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 39/50 [14:15<04:03, 22.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 38 ------------------------------\n",
            "rewards/mean:\t0.139375091\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.005866603\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.519901156\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 40/50 [14:36<03:36, 21.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 39 ------------------------------\n",
            "rewards/mean:\t0.194767654\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t-0.024233123\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.464358151\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▏ | 41/50 [14:57<03:12, 21.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 40 ------------------------------\n",
            "rewards/mean:\t0.252348512\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.031494543\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.585824132\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 42/50 [15:19<02:52, 21.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 41 ------------------------------\n",
            "rewards/mean:\t0.737719119\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.095232829\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.602477491\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 43/50 [15:39<02:27, 21.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 42 ------------------------------\n",
            "rewards/mean:\t0.348758221\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.042668916\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.473462075\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 44/50 [16:00<02:07, 21.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 43 ------------------------------\n",
            "rewards/mean:\t-0.270424843\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t-0.084525675\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.590520024\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 45/50 [16:22<01:47, 21.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 44 ------------------------------\n",
            "rewards/mean:\t0.355695844\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.022175774\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.589467466\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 46/50 [16:43<01:25, 21.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 45 ------------------------------\n",
            "rewards/mean:\t0.687814593\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.144094676\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.588263392\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|█████████▍| 47/50 [17:05<01:04, 21.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 46 ------------------------------\n",
            "rewards/mean:\t0.874625981\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.111767992\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.645615757\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 48/50 [17:26<00:42, 21.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 47 ------------------------------\n",
            "rewards/mean:\t0.925737619\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.166211829\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.396295190\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 49/50 [17:48<00:21, 21.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 48 ------------------------------\n",
            "rewards/mean:\t1.073199272\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.179797187\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.731850266\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [18:12<00:00, 21.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 49 ------------------------------\n",
            "rewards/mean:\t0.493699908\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.101782501\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.935232162\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "max_steps = 50   # can be insufficient for some tasks - watch your learning curves\n",
        "generation_kwargs = dict(\n",
        "    min_length=-1, max_new_tokens=128, do_sample=True, top_k=0, top_p=1.0, pad_token_id=main_tokenizer.eos_token_id)\n",
        "#                                  ^-- task-specific parameter!\n",
        "with tqdm(enumerate(ppo_trainer.dataloader), total=max_steps) as progressbar:\n",
        "  # note: ppo_trainer.dataloader is just a regular dataloader of queries, no RL-specific magic :)\n",
        "  for epoch, batch in progressbar:\n",
        "    if epoch >= max_steps:\n",
        "        break\n",
        "\n",
        "    # Rollout stage: generate continuations from batch queries using main_model\n",
        "    response_tensors = ppo_trainer.generate(batch['input_ids'], **generation_kwargs)\n",
        "    # ^-- list of tensors of token ids from main model tokenizer\n",
        "\n",
        "    # de-tokenize responses to strings (since reward model uses a different tokenizer)\n",
        "    batch[\"response\"] = [main_tokenizer.decode(response.squeeze()) for response in response_tensors]\n",
        "    # note: response_tensors already contain query tokens, so we don't need to add queries manually.\n",
        "    # This may not be true for other tasks: check this manually by viewing batch[\"response\"] and batch[\"query\"]\n",
        "\n",
        "\n",
        "    # Evaluation stage\n",
        "    rewards = compute_reward(batch['response'])\n",
        "\n",
        "    # Update stage\n",
        "    stats = ppo_trainer.step(batch['input_ids'], response_tensors, list(rewards.split(1)))\n",
        "    stats['rewards/mean'] = rewards.mean().item()\n",
        "\n",
        "    print(\"-\" * 30, 'STEP', epoch, '-' * 30)\n",
        "    print(f'rewards/mean:\\t{stats[\"rewards/mean\"]:.9f}\\t<---- average reward over this batch (higher=better, noisy)')\n",
        "    print(f'ppo/returns/mean:\\t{stats[\"ppo/returns/mean\"]:.9f}\\t<---- model-estimated average discounted reward')\n",
        "    print(f'objective/kl:\\t{stats[\"objective/kl\"]:.9f}\\t<---- how far we are from the original model (regularizer)')\n",
        "    print()\n",
        "\n",
        "    ppo_trainer.log_stats(stats, batch, list(rewards.split(1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hgtmjtilq6T8"
      },
      "source": [
        "## Main assignment - <u>actually</u> train the model (8 points)\n",
        "\n",
        "\n",
        "Your main task for this week is to use the RLHF pipeline to train a model for a reward of your choice. Here's what you can choose from:\n",
        "\n",
        "__A. Toxicity fine-tuning:__ train the model to be less (or more!) toxic. For this task, you may use the data from [jigsaw toxic comments](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) and [lmsys/toxic-chat](https://huggingface.co/datasets/lmsys/toxic-chat),  or any other source. Alternatively, you may use toxicity scores from [oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1).\n",
        "\n",
        "\n",
        "__B. Actual human feedback:__ use one of the existing datasets with pairwise human feedback to align your langauge model. You may use [anthropic's hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf), [OpenAssistant dataset](https://huggingface.co/datasets/OpenAssistant/oasst1) or any other data you see fit. You may also turn the tables and train the model to [minimize](https://habrastorage.org/getpro/geektimes/post_images/ac7/2ad/827/ac72ad82767d4132164a4b6b76196c42.jpg) human preferences, as long as your model does not degrade to gibberish.\n",
        "\n",
        "__C. Controlled generation:__ Instead of training a reward model from human feedback, you may define the reward function as the text length (longer or shorter) or number of times the model uses specific words (e.g. \"sorry\", \"apologize\"). If you choose specific words, make sure the model generates them at least sometimes.\n",
        "\n",
        "__Alternatively,__ you may choose a different task. However, unless your task is very similar to one of the above, there is a chance that it will be **significantly** harder to solve, requiring orders of magnitude more compute and tuning. If you are in doubt, please ask the course staff. If they are AFK (again >.<), please prefer one of the recommended tasks.\n",
        "\n",
        "\n",
        "#### General tips & tricks\n",
        "\n",
        "\n",
        "Things to look out for:\n",
        "- during PPO stage, the reward model should be in eval mode (dropout disabled)\n",
        "- make sure max_length and max_new_tokens are enough for your chosen dataset - at least most of the time\n",
        "- when in doubt, view the data manually or inspect how the model performs on a few samples\n",
        "\n",
        "\n",
        "We highly recommend that you manually check the performance after each sub-stage:\n",
        "1. when you assembled the pairwise dataset, inspect a couple of from of *your* dataset class and detokenize them. Make sure that you-the-human understand why one sample was accepted and the other - rejected. At least most of the time. This also lets you spot tokenization/truncation errors.\n",
        "2. after you trained a reward model, measure how accurate this model is in isolation. If your reward model is poor, any subsequent RLHF will also fail.\n",
        "3. once you've trained the main model with RL, ask it to generate examples and explore how well it does. If it produces an obviously bad output, check if the reward model assigns high reward to that output. If yes, reward model is the culprit; if no, it's a question of better/longer PPO training.\n",
        "\n",
        "__It is also a good idea to periodically print samples during training.__\n",
        "\n",
        "__When stuck, simplify the problem.__ If you've spent a several hours enchanting the reward model but it still won't budge, try switching to a simple subtask. For instance, if you're training on hh-rlhf, try limiting it the dataset to 10% of the shortest sequences - they are typically easier to learn.\n",
        "\n",
        "\n",
        "## Assignment stages (and grading)\n",
        "\n",
        "Regardless of the specific task you chose, your solution needs to contain several parts that will be graded separately.\n",
        "\n",
        "\n",
        "#### Stage 1: reward model (4 points)\n",
        "\n",
        "Construct a dataset for training the reward model on your problem. Then, train a reward model on that dataset and evaluate how well can your model predict preferences on a hold-out (test) subset of your data.\n",
        "\n",
        "Please make sure that the part of your notebook where you evaluate reward model is clearly visible and reasonably easy to read. And for all that is holy, do not call it IMDB unless it actually **is** data of imdb movie reviews :)\n",
        "\n",
        "__Not all tasks require a reward model for later PPO fine-tuning.__ For instance, there's no reason to train a reward model if your reward equals sentence length. Likewise, toxicity reward can be estimated with a pre-trained toxicity classifier. __If your task does not require training a reward model, please train an unrelated model on [hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf) as though you were solving assignment version B.__ This is for grading purposes only, you won't use this model for stage 2.\n",
        "\n",
        "\n",
        "#### Stage 2: RL fine-tuning (4 points)\n",
        "\n",
        "Once the reward model is ready - or you can compute rewards without a model - it is time to maximize that reward with PPO. Optionally, you may replace PPO with another RL algorithm (or unlikelihood learning scheme), but only if you're feeling adventurous.\n",
        "\n",
        "\n",
        "First, you need to choose a language model to be fine-tuned. You may choose any model, but make sure that your model **can** generate the data in your format. For instance, [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) is a general purpose LM and may (or may not) need prompt engineering to generate chat assistant responses. For that reason, it is best if you **do not use `\"lvwerra/gpt2-imdb\"` unless you're generating only movie reviews**.\n",
        "\n",
        "\n",
        "\n",
        "There are two \"difficulty modes\" for this task:\n",
        "For the **easy mode**, use [gpt2-large](https://huggingface.co/gpt2-large) or [opt-1.3b](https://huggingface.co/facebook/opt-1.3b) with minimal code changes.\n",
        "If you want the **Hard mode:** use a larger (e.g. 7B) model in combination with `load_in_4bit` and LoRA, the same way we did last week.\n",
        "Some reasonable model choices are [LLaMA-7B](https://huggingface.co/Enoch/llama-7b-hf), [Falcon-7b](https://huggingface.co/tiiuae/falcon-7b), [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) for general-purpose LM or [guanaco-7b](https://huggingface.co/timdettmers/guanaco-7b), [vicuna-7b](https://huggingface.co/lmsys/vicuna-7b-v1.5) for chat-based tasks, though there are many more (see [leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)). In the hard mode, you will need to modify the training arguments to enable 4-bit fine-tuning. Furthermore, your experiments will take somewhat longer to complete. On the plus side, your model will produce significantly better results.\n",
        "\n",
        "__High reward is not enough!__ RL algorithms are famous for [cheating their reward functions](https://openai.com/research/faulty-reward-functions). To ensure that your model is actually doing what you want it to do, you will need some additional evaluation. To get the full grade, provide at least 20 side-by-side examples of your fine-tuned model vs original model predictions and a short summary.\n",
        "\n",
        "Alternatively, you may provide 5 examples and some extrinsic evaluation metric over many examples. For instance, you may use a different pre-trained toxicity score for option A. When dealing with human preferences, you may choose to [enlist actual humans](https://toloka.ai/) or [ask GPT4/Claude](https://arxiv.org/pdf/2304.03277.pdf) to compare your model's predictions. For task C, when optimizing for simple rewards like sentence lengths, it is enough to compare histograms of rewards (e.g. average lengths).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"lmsys/toxic-chat\", \"toxicchat0124\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import datasets\n",
        "\n",
        "class FeaturedDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"A dataset with pairwise ranked numerical features\"\"\"\n",
        "    def __init__(self, ds, tokenizer, target_feature: int):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.ds = ds\n",
        "        self.target_feature = target_feature\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)**2  # all pairs\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        sample = self.ds[index//len(self.ds)]\n",
        "        another = self.ds[index%len(self.ds)]\n",
        "        if dict(eval(sample['openai_moderation']))[self.target_feature] > dict(eval(another['openai_moderation']))[self.target_feature]:\n",
        "            chosen = self.tokenizer(sample['model_output'], truncation=True)\n",
        "            rejected = self.tokenizer(another['model_output'], truncation=True)\n",
        "        else:\n",
        "            chosen = self.tokenizer(another['model_output'], truncation=True)\n",
        "            rejected = self.tokenizer(sample['model_output'], truncation=True)\n",
        "            \n",
        "        return dict(input_ids_chosen=chosen['input_ids'], attention_mask_chosen=chosen['attention_mask'],\n",
        "                    input_ids_rejected=rejected['input_ids'], attention_mask_rejected=rejected['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import transformers\n",
        "device = 'cuda'\n",
        "reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", device_map=device)\n",
        "reward_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "reward_data = FeaturedDataset(ds['train'],reward_tokenizer,'sexual')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
            "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
            "the same time. Both libraries are known to be incompatible and this\n",
            "can cause random crashes or deadlocks on Linux when loaded in the\n",
            "same Python program.\n",
            "Using threadpoolctl may cause crashes or deadlocks. For more\n",
            "information and possible workarounds, please see\n",
            "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
            "\n",
            "  warnings.warn(msg, RuntimeWarning)\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\reward_trainer.py:182: UserWarning: When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig. It will be set to `512` by default, but you should do it yourself in the future.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\reward_trainer.py:199: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "  2%|▎         | 50/2000 [03:44<2:25:50,  4.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6349, 'grad_norm': 1.9936622381210327, 'learning_rate': 1.95e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 100/2000 [07:57<2:28:29,  4.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.553, 'grad_norm': 2.1092488765716553, 'learning_rate': 1.9010000000000003e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 150/2000 [11:50<2:17:55,  4.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4982, 'grad_norm': 1.9142942428588867, 'learning_rate': 1.851e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 200/2000 [15:32<2:10:41,  4.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4434, 'grad_norm': 2.496483087539673, 'learning_rate': 1.8010000000000002e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▎        | 250/2000 [19:14<2:12:15,  4.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3843, 'grad_norm': 2.6583919525146484, 'learning_rate': 1.751e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 300/2000 [23:11<2:28:11,  5.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.347, 'grad_norm': 2.926581621170044, 'learning_rate': 1.701e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 350/2000 [27:05<2:06:17,  4.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.321, 'grad_norm': 3.3935976028442383, 'learning_rate': 1.6510000000000003e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 400/2000 [31:12<2:00:32,  4.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2893, 'grad_norm': 2.8093655109405518, 'learning_rate': 1.601e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▎       | 450/2000 [34:53<1:52:07,  4.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2828, 'grad_norm': 3.060565948486328, 'learning_rate': 1.5510000000000002e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 500/2000 [38:51<1:53:17,  4.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2587, 'grad_norm': 2.9939539432525635, 'learning_rate': 1.501e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
            " 28%|██▊       | 550/2000 [42:38<1:45:31,  4.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2447, 'grad_norm': 2.8916547298431396, 'learning_rate': 1.4510000000000002e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 600/2000 [46:17<1:41:01,  4.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.233, 'grad_norm': 2.8077616691589355, 'learning_rate': 1.4010000000000001e-05, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▎      | 650/2000 [50:05<2:05:20,  5.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2274, 'grad_norm': 2.9370219707489014, 'learning_rate': 1.3510000000000001e-05, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 700/2000 [53:52<1:34:52,  4.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2238, 'grad_norm': 2.917951822280884, 'learning_rate': 1.301e-05, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 750/2000 [57:46<1:50:56,  5.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.206, 'grad_norm': 3.9415810108184814, 'learning_rate': 1.251e-05, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 800/2000 [1:01:24<1:26:50,  4.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2012, 'grad_norm': 3.655064582824707, 'learning_rate': 1.2010000000000002e-05, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▎     | 850/2000 [1:05:08<1:25:14,  4.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1942, 'grad_norm': 3.060561418533325, 'learning_rate': 1.1510000000000002e-05, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 900/2000 [1:08:57<1:20:02,  4.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1955, 'grad_norm': 2.7942662239074707, 'learning_rate': 1.1010000000000001e-05, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 950/2000 [1:12:43<1:23:26,  4.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1868, 'grad_norm': 2.633648633956909, 'learning_rate': 1.0510000000000001e-05, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1000/2000 [1:16:54<1:17:03,  4.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1875, 'grad_norm': 3.1920478343963623, 'learning_rate': 1.0009999999999999e-05, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
            " 52%|█████▎    | 1050/2000 [1:20:50<1:13:08,  4.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1806, 'grad_norm': 3.432727098464966, 'learning_rate': 9.51e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 1100/2000 [1:24:49<1:18:38,  5.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1758, 'grad_norm': 3.018944263458252, 'learning_rate': 9.01e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▊    | 1150/2000 [1:28:46<1:04:17,  4.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1757, 'grad_norm': 3.684968948364258, 'learning_rate': 8.51e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 1200/2000 [1:32:44<1:01:04,  4.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1665, 'grad_norm': 2.986323356628418, 'learning_rate': 8.010000000000001e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▎   | 1250/2000 [1:36:36<1:03:14,  5.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1629, 'grad_norm': 3.572213888168335, 'learning_rate': 7.510000000000001e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 1300/2000 [1:40:38<54:46,  4.70s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1637, 'grad_norm': 3.561124086380005, 'learning_rate': 7.01e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 1350/2000 [1:44:26<49:15,  4.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1609, 'grad_norm': 2.7606916427612305, 'learning_rate': 6.51e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 1400/2000 [1:48:35<48:57,  4.90s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1542, 'grad_norm': 3.6478464603424072, 'learning_rate': 6.01e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▎  | 1450/2000 [1:52:53<53:55,  5.88s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1585, 'grad_norm': 2.8350319862365723, 'learning_rate': 5.510000000000001e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▌  | 1500/2000 [1:56:43<37:45,  4.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1484, 'grad_norm': 3.2492713928222656, 'learning_rate': 5.01e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
            " 78%|███████▊  | 1550/2000 [2:00:59<34:19,  4.58s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.155, 'grad_norm': 3.4183096885681152, 'learning_rate': 4.510000000000001e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 1600/2000 [2:05:00<32:37,  4.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1461, 'grad_norm': 3.3123769760131836, 'learning_rate': 4.0100000000000006e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▎ | 1650/2000 [2:09:02<37:30,  6.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.148, 'grad_norm': 3.708418369293213, 'learning_rate': 3.5100000000000003e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 1700/2000 [2:13:04<33:27,  6.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1424, 'grad_norm': 2.6902918815612793, 'learning_rate': 3.01e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 1750/2000 [2:16:58<19:45,  4.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1435, 'grad_norm': 3.617710590362549, 'learning_rate': 2.51e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 1800/2000 [2:21:00<15:30,  4.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1386, 'grad_norm': 3.330132246017456, 'learning_rate': 2.0100000000000002e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▎| 1850/2000 [2:25:26<11:24,  4.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.142, 'grad_norm': 3.7916035652160645, 'learning_rate': 1.5100000000000002e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 1900/2000 [2:29:13<07:31,  4.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1311, 'grad_norm': 3.591104507446289, 'learning_rate': 1.01e-06, 'epoch': 0.01}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 1950/2000 [2:33:42<03:47,  4.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1388, 'grad_norm': 2.7564380168914795, 'learning_rate': 5.1e-07, 'epoch': 0.02}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [2:37:41<00:00,  4.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1377, 'grad_norm': 3.4875423908233643, 'learning_rate': 1e-08, 'epoch': 0.02}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [2:37:42<00:00,  4.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 9462.5938, 'train_samples_per_second': 42.272, 'train_steps_per_second': 0.211, 'train_loss': 0.229576851606369, 'epoch': 0.02}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2000, training_loss=0.229576851606369, metrics={'train_runtime': 9462.5938, 'train_samples_per_second': 42.272, 'train_steps_per_second': 0.211, 'total_flos': 0.0, 'train_loss': 0.229576851606369, 'epoch': 0.015487787879257206})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import trl\n",
        "\n",
        "training_args = trl.RewardConfig( # like transformers.TrainingArguments\n",
        "    output_dir=\"r_model\",\n",
        "    per_device_train_batch_size=100,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-5,\n",
        "    max_steps=2000,            # note: training may need more than 1k steps\n",
        "    logging_steps=50,\n",
        "    gradient_checkpointing=True,  # reduce memory usage but train ~30% slower\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    fp16=True,\n",
        "    dataset_num_proc=11,\n",
        "    \n",
        ")\n",
        "\n",
        "trainer = trl.RewardTrainer(\n",
        "    model=reward_model,\n",
        "    args=training_args,\n",
        "    tokenizer=reward_tokenizer,\n",
        "    train_dataset=reward_data,\n",
        "    peft_config=None,  # optionally, you may tune with LoRA, prompt-tuning, etc\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"lmsys/toxic-chat\", \"toxicchat0124\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "device = 'cuda'\n",
        "reward_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(\"r_model/checkpoint-2000\", device_map=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import trl\n",
        "import torch\n",
        "import transformers\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "qconfig = transformers.BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_use_double_quant=True)\n",
        "main_tokenizer = transformers.AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")\n",
        "main_tokenizer.pad_token = main_tokenizer.eos_token\n",
        "main_model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(\"openai-community/gpt2-large\", device_map=device,quantization_config = qconfig,torch_dtype = torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import get_peft_model,LoraConfig,TaskType\n",
        "lora_config = LoraConfig(\n",
        "                target_modules=[\"c_attn\",'c_proj','c_fc','lm_head'],\n",
        "                lora_alpha=32,\n",
        "                inference_mode=False,\n",
        "                use_rslora = True,\n",
        "                bias = 'all',\n",
        "                lora_dropout=0.,\n",
        "                r=4,\n",
        "                task_type=TaskType.CAUSAL_LM,\n",
        "                init_lora_weights='gaussian'\n",
        "            )\n",
        "main_model = get_peft_model(main_model,lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 3,663,429 || all params: 777,186,629 || trainable%: 0.4714\n"
          ]
        }
      ],
      "source": [
        "main_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import trl\n",
        "# Note: this code is specific to IMDB; you will need to re-write it for other tasks\n",
        "sample_length = trl.core.LengthSampler(2, 8)  # use the first 2-8 tokens as query\n",
        "\n",
        "def select_query_and_tokenize(sample):\n",
        "    query_ids = main_tokenizer.encode(sample[\"user_input\"])[:sample_length()]\n",
        "    sample[\"query\"] = main_tokenizer.decode(query_ids)  # query is the only required column\n",
        "    sample[\"input_ids\"] = query_ids  # to avoid re-tokenizing later\n",
        "    return sample  # we do not need the rest - it will be generated by the model\n",
        "\n",
        "ds_rlhf = ds['test'].map(select_query_and_tokenize, batched=False)\n",
        "ds_rlhf.set_format(type=\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
            "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
            "the same time. Both libraries are known to be incompatible and this\n",
            "can cause random crashes or deadlocks on Linux when loaded in the\n",
            "same Python program.\n",
            "Using threadpoolctl may cause crashes or deadlocks. For more\n",
            "information and possible workarounds, please see\n",
            "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
            "\n",
            "  warnings.warn(msg, RuntimeWarning)\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import trl\n",
        "training_args = trl.PPOConfig(\n",
        "    model_name=main_model.config._name_or_path,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-5,\n",
        "    batch_size=32,\n",
        "    mini_batch_size=8,\n",
        "    ppo_epochs=4,\n",
        "    dataset_num_proc=8,\n",
        "              # PPO performs this many updates per training batch\n",
        ")\n",
        "\n",
        "ppo_trainer = trl.PPOTrainer(\n",
        "    training_args, model=main_model.model, tokenizer=main_tokenizer,\n",
        "    dataset=ds_rlhf, data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0]),\n",
        ")  # note: we pass main_model.model because PPOTrainer checks for one of several supported model types ...\n",
        "# ... main_model.model is a model with adapters, which is supported. main_model itself is a wrapper that is not supported"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "def compute_reward(texts: List[str]) -> torch.Tensor:\n",
        "  inputs = reward_tokenizer(texts, truncation=True, padding=True, return_tensors='pt').to(device)\n",
        "  with torch.no_grad():\n",
        "    return reward_model(**inputs).logits[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/150 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "  1%|          | 1/150 [01:03<2:37:43, 63.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 0 ------------------------------\n",
            "rewards/mean:\t1.341312528\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.226613402\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.000000000\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 2/150 [02:15<2:49:08, 68.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 1 ------------------------------\n",
            "rewards/mean:\t1.314770699\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.319299579\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t-0.634562492\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.27 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "  2%|▏         | 3/150 [03:24<2:47:50, 68.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 2 ------------------------------\n",
            "rewards/mean:\t1.233374119\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.466379970\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t-1.265505314\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 4/150 [04:29<2:43:40, 67.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 3 ------------------------------\n",
            "rewards/mean:\t1.979054451\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.677819371\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.299598038\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 5/150 [05:38<2:44:12, 67.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 4 ------------------------------\n",
            "rewards/mean:\t1.392532110\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.762321830\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.991976619\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 6/150 [06:47<2:43:55, 68.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 5 ------------------------------\n",
            "rewards/mean:\t2.719542503\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.039131522\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t0.826579690\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▍         | 7/150 [07:55<2:42:24, 68.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 6 ------------------------------\n",
            "rewards/mean:\t1.877263546\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.020757914\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t3.831849575\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 8/150 [08:58<2:37:37, 66.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 7 ------------------------------\n",
            "rewards/mean:\t1.731281519\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.048143148\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t6.026952744\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 9/150 [10:06<2:37:34, 67.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 8 ------------------------------\n",
            "rewards/mean:\t2.312290907\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.136253119\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t5.247801304\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 10/150 [11:10<2:34:23, 66.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 9 ------------------------------\n",
            "rewards/mean:\t1.951788783\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.090348482\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t5.103988171\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 11/150 [12:18<2:34:34, 66.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 10 ------------------------------\n",
            "rewards/mean:\t0.812562227\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.898574233\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t4.089690685\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 12/150 [13:24<2:32:27, 66.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 11 ------------------------------\n",
            "rewards/mean:\t2.612108707\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.079362035\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t7.723804474\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▊         | 13/150 [14:28<2:29:53, 65.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 12 ------------------------------\n",
            "rewards/mean:\t1.088492632\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.764500320\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t7.949388981\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 14/150 [15:37<2:31:22, 66.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 13 ------------------------------\n",
            "rewards/mean:\t3.501738548\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.361379981\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t9.644536972\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 15/150 [16:43<2:29:45, 66.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 14 ------------------------------\n",
            "rewards/mean:\t3.466894388\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.260950446\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t9.719232559\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 16/150 [17:52<2:30:21, 67.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 15 ------------------------------\n",
            "rewards/mean:\t2.914625168\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.144826531\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t11.818075180\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█▏        | 17/150 [19:04<2:31:49, 68.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 16 ------------------------------\n",
            "rewards/mean:\t3.355872154\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.135199308\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t9.554873466\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 18/150 [20:09<2:28:23, 67.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 17 ------------------------------\n",
            "rewards/mean:\t3.872513056\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.383689523\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t10.846442223\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 19/150 [21:19<2:29:19, 68.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 18 ------------------------------\n",
            "rewards/mean:\t3.645012140\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.338815928\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t12.541727066\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 20/150 [22:33<2:31:29, 69.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 19 ------------------------------\n",
            "rewards/mean:\t2.355683327\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.241760850\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t13.797780037\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 21/150 [23:44<2:31:22, 70.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 20 ------------------------------\n",
            "rewards/mean:\t3.898432016\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.661746025\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.300355911\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▍        | 22/150 [24:53<2:28:50, 69.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 21 ------------------------------\n",
            "rewards/mean:\t5.079894066\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.822383523\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.491298676\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 23/150 [26:05<2:29:40, 70.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 22 ------------------------------\n",
            "rewards/mean:\t3.485247612\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.542023897\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.192256927\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 24/150 [27:16<2:28:16, 70.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 23 ------------------------------\n",
            "rewards/mean:\t4.860739708\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.600691676\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.574884415\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 25/150 [28:30<2:29:14, 71.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 24 ------------------------------\n",
            "rewards/mean:\t5.452943802\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.332481384\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t23.163593292\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 26/150 [29:40<2:27:21, 71.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 25 ------------------------------\n",
            "rewards/mean:\t3.536050558\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.345178843\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.190950394\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 27/150 [30:50<2:25:08, 70.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 26 ------------------------------\n",
            "rewards/mean:\t5.425052643\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.897816420\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.492385864\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▊        | 28/150 [32:03<2:25:00, 71.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 27 ------------------------------\n",
            "rewards/mean:\t4.559685707\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.911891699\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.495073318\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 29/150 [33:14<2:23:38, 71.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 28 ------------------------------\n",
            "rewards/mean:\t5.250615120\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.917570710\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.804821014\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 30/150 [34:23<2:21:13, 70.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 29 ------------------------------\n",
            "rewards/mean:\t5.997655869\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.242083073\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t22.009107590\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 31/150 [35:28<2:16:49, 68.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 30 ------------------------------\n",
            "rewards/mean:\t6.340649605\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.051354885\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.054113388\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██▏       | 32/150 [36:35<2:14:23, 68.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 31 ------------------------------\n",
            "rewards/mean:\t5.862373352\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.834059715\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.961746216\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 33/150 [37:41<2:11:57, 67.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 32 ------------------------------\n",
            "rewards/mean:\t7.908440590\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t3.315105677\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.547580719\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 34/150 [38:54<2:14:04, 69.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 33 ------------------------------\n",
            "rewards/mean:\t6.929636478\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t3.274160624\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.270620346\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 35/150 [40:03<2:12:29, 69.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 34 ------------------------------\n",
            "rewards/mean:\t7.966818810\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t3.354738712\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.693359375\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 36/150 [41:08<2:08:53, 67.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 35 ------------------------------\n",
            "rewards/mean:\t5.848605156\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.427324772\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.464336395\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▍       | 37/150 [42:17<2:08:39, 68.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 36 ------------------------------\n",
            "rewards/mean:\t7.071865559\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t3.490811825\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.304241180\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 38/150 [43:23<2:05:59, 67.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 37 ------------------------------\n",
            "rewards/mean:\t6.002763748\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.609704256\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.964504242\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 39/150 [44:16<1:57:15, 63.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 38 ------------------------------\n",
            "rewards/mean:\t4.815419197\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.645116091\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t12.653374672\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 40/150 [45:04<1:47:35, 58.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 39 ------------------------------\n",
            "rewards/mean:\t4.575706482\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.280853510\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t10.536380768\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 41/150 [45:59<1:44:44, 57.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 40 ------------------------------\n",
            "rewards/mean:\t5.476653099\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.204080343\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t15.210311890\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 42/150 [46:54<1:42:26, 56.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 41 ------------------------------\n",
            "rewards/mean:\t2.541163445\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t0.659190595\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t11.902305603\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▊       | 43/150 [48:00<1:46:06, 59.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 42 ------------------------------\n",
            "rewards/mean:\t5.004567623\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.921865940\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.280464172\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▉       | 44/150 [48:59<1:44:38, 59.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 43 ------------------------------\n",
            "rewards/mean:\t5.070037365\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.800494909\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t13.173219681\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 45/150 [49:50<1:39:33, 56.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 44 ------------------------------\n",
            "rewards/mean:\t6.089390755\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.056863308\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t13.403395653\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|███       | 46/150 [50:48<1:39:05, 57.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 45 ------------------------------\n",
            "rewards/mean:\t5.657330513\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.290950060\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.486110687\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|███▏      | 47/150 [51:46<1:38:23, 57.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 46 ------------------------------\n",
            "rewards/mean:\t6.434219360\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.200557947\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t15.126501083\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 48/150 [52:23<1:27:33, 51.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 47 ------------------------------\n",
            "rewards/mean:\t6.499149323\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.669670343\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t14.407856941\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 49/150 [53:11<1:24:48, 50.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 48 ------------------------------\n",
            "rewards/mean:\t5.367022991\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.727546692\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t15.238353729\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 50/150 [53:58<1:22:07, 49.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 49 ------------------------------\n",
            "rewards/mean:\t6.055223465\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.767988205\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.585762024\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 51/150 [54:51<1:23:03, 50.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 50 ------------------------------\n",
            "rewards/mean:\t7.319556236\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.970761776\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.437261581\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▍      | 52/150 [55:28<1:15:47, 46.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 51 ------------------------------\n",
            "rewards/mean:\t6.875696182\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t3.381352425\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.067337036\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 53/150 [56:13<1:14:14, 45.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 52 ------------------------------\n",
            "rewards/mean:\t6.359442711\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.316426992\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.919857025\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 54/150 [57:09<1:18:23, 49.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 53 ------------------------------\n",
            "rewards/mean:\t5.822316647\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.006809711\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.558523178\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 55/150 [57:52<1:14:58, 47.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 54 ------------------------------\n",
            "rewards/mean:\t7.191326618\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.268063068\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.028964996\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 56/150 [58:33<1:11:10, 45.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 55 ------------------------------\n",
            "rewards/mean:\t6.783601761\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t1.986341000\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.058065414\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 57/150 [59:14<1:08:23, 44.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 56 ------------------------------\n",
            "rewards/mean:\t7.796194077\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t2.780905724\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t21.820026398\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▊      | 58/150 [59:49<1:03:02, 41.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 57 ------------------------------\n",
            "rewards/mean:\t7.973384380\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t3.570426226\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t21.094982147\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 59/150 [1:00:19<57:28, 37.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 58 ------------------------------\n",
            "rewards/mean:\t7.144409180\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t3.212305069\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.118520737\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 60/150 [1:00:44<51:17, 34.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 59 ------------------------------\n",
            "rewards/mean:\t8.634210587\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t4.239290714\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t21.633682251\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████      | 61/150 [1:01:09<46:18, 31.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 60 ------------------------------\n",
            "rewards/mean:\t8.232854843\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t4.682798386\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.605241776\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████▏     | 62/150 [1:01:39<45:22, 30.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 61 ------------------------------\n",
            "rewards/mean:\t7.992423058\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t4.087344170\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t22.157684326\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 63/150 [1:02:09<44:18, 30.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 62 ------------------------------\n",
            "rewards/mean:\t7.739045143\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t4.350378036\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.495399475\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 64/150 [1:02:38<43:22, 30.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 63 ------------------------------\n",
            "rewards/mean:\t7.686486244\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t3.883570194\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.496810913\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 65/150 [1:03:03<40:41, 28.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 64 ------------------------------\n",
            "rewards/mean:\t8.509353638\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t4.820219040\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.676715851\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 66/150 [1:03:25<37:19, 26.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 65 ------------------------------\n",
            "rewards/mean:\t8.144248009\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t4.500892162\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.936874390\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▍     | 67/150 [1:03:43<33:05, 23.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 66 ------------------------------\n",
            "rewards/mean:\t8.824471474\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.517116547\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t21.117475510\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 68/150 [1:04:03<31:01, 22.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 67 ------------------------------\n",
            "rewards/mean:\t7.739512920\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t4.957062244\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.338497162\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 69/150 [1:04:22<29:07, 21.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 68 ------------------------------\n",
            "rewards/mean:\t8.438900948\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.258892536\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.890731812\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 70/150 [1:04:38<26:48, 20.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 69 ------------------------------\n",
            "rewards/mean:\t8.438972473\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.372510910\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.483896255\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 71/150 [1:04:59<26:49, 20.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 70 ------------------------------\n",
            "rewards/mean:\t8.646898270\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.685758591\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t21.410249710\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 72/150 [1:05:17<25:22, 19.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 71 ------------------------------\n",
            "rewards/mean:\t8.387558937\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.631323338\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t21.656536102\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████▊     | 73/150 [1:05:31<23:08, 18.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 72 ------------------------------\n",
            "rewards/mean:\t7.881222725\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.434672832\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.791301727\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 74/150 [1:05:48<22:08, 17.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 73 ------------------------------\n",
            "rewards/mean:\t7.798803329\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.134981155\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t21.409873962\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 75/150 [1:06:04<21:24, 17.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 74 ------------------------------\n",
            "rewards/mean:\t9.091558456\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.987581730\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t23.649581909\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|█████     | 76/150 [1:06:23<21:53, 17.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 75 ------------------------------\n",
            "rewards/mean:\t9.141706467\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t6.024496078\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t22.714954376\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|█████▏    | 77/150 [1:06:39<20:50, 17.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 76 ------------------------------\n",
            "rewards/mean:\t8.814638138\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.862103462\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t22.515727997\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 78/150 [1:06:53<19:27, 16.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 77 ------------------------------\n",
            "rewards/mean:\t8.790559769\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.840145588\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t21.581649780\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 79/150 [1:07:06<18:16, 15.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 78 ------------------------------\n",
            "rewards/mean:\t8.399183273\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.547458649\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t21.029064178\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 80/150 [1:07:21<17:37, 15.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 79 ------------------------------\n",
            "rewards/mean:\t8.705688477\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.696536064\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t22.096145630\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|█████▍    | 81/150 [1:07:36<17:27, 15.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 80 ------------------------------\n",
            "rewards/mean:\t8.633142471\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.941682816\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.737798691\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▍    | 82/150 [1:07:48<16:05, 14.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 81 ------------------------------\n",
            "rewards/mean:\t8.386959076\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.881262779\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.330665588\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 83/150 [1:07:59<14:50, 13.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 82 ------------------------------\n",
            "rewards/mean:\t7.877856255\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.272432804\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.980041504\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 84/150 [1:08:12<14:29, 13.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 83 ------------------------------\n",
            "rewards/mean:\t8.918150902\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.850790501\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.955596924\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 85/150 [1:08:23<13:34, 12.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 84 ------------------------------\n",
            "rewards/mean:\t8.463045120\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.736501694\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.504901886\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 86/150 [1:08:33<12:36, 11.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 85 ------------------------------\n",
            "rewards/mean:\t8.791949272\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.927683353\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.306018829\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 87/150 [1:08:46<12:40, 12.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 86 ------------------------------\n",
            "rewards/mean:\t9.019742012\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t6.006408691\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t22.459373474\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|█████▊    | 88/150 [1:09:00<12:59, 12.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 87 ------------------------------\n",
            "rewards/mean:\t8.310916901\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.439692974\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.441547394\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|█████▉    | 89/150 [1:09:13<13:05, 12.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 88 ------------------------------\n",
            "rewards/mean:\t8.414443970\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.451375484\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.553842545\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 90/150 [1:09:26<12:58, 12.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 89 ------------------------------\n",
            "rewards/mean:\t8.657321930\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.997351646\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.276069641\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:1246: UserWarning: The average ratio of batch (29.25) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            " 61%|██████    | 91/150 [1:09:38<12:22, 12.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 90 ------------------------------\n",
            "rewards/mean:\t7.805204391\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.325268745\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.523683548\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████▏   | 92/150 [1:09:49<11:43, 12.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 91 ------------------------------\n",
            "rewards/mean:\t9.023410797\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t6.133665085\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.928413391\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 93/150 [1:10:01<11:28, 12.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 92 ------------------------------\n",
            "rewards/mean:\t8.460382462\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.660878181\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.796623230\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 94/150 [1:10:15<11:44, 12.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 93 ------------------------------\n",
            "rewards/mean:\t7.179310799\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.059085846\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t16.553480148\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 95/150 [1:10:26<11:06, 12.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 94 ------------------------------\n",
            "rewards/mean:\t8.196863174\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.603809357\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t16.895820618\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 96/150 [1:10:42<11:57, 13.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 95 ------------------------------\n",
            "rewards/mean:\t9.010677338\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.735694885\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t22.168397903\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▍   | 97/150 [1:10:53<11:15, 12.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 96 ------------------------------\n",
            "rewards/mean:\t8.691274643\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.721870422\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.798976898\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 98/150 [1:11:09<11:51, 13.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 97 ------------------------------\n",
            "rewards/mean:\t8.824056625\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.709155083\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.440044403\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|██████▌   | 99/150 [1:11:20<10:48, 12.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 98 ------------------------------\n",
            "rewards/mean:\t8.489061356\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.653582573\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.735649109\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 100/150 [1:11:32<10:30, 12.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 99 ------------------------------\n",
            "rewards/mean:\t8.527273178\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.467230797\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.574249268\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 101/150 [1:11:43<09:44, 11.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 100 ------------------------------\n",
            "rewards/mean:\t8.474880219\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t6.012085915\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t15.097265244\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 102/150 [1:11:56<09:49, 12.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 101 ------------------------------\n",
            "rewards/mean:\t8.761706352\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.986245155\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.656660080\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▊   | 103/150 [1:12:07<09:26, 12.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 102 ------------------------------\n",
            "rewards/mean:\t8.260752678\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.327812195\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.389867783\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 104/150 [1:12:17<08:50, 11.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 103 ------------------------------\n",
            "rewards/mean:\t8.487174988\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.798085213\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.302696228\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 105/150 [1:12:29<08:32, 11.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 104 ------------------------------\n",
            "rewards/mean:\t8.238833427\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.304841518\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.418672562\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████   | 106/150 [1:12:39<08:05, 11.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 105 ------------------------------\n",
            "rewards/mean:\t8.563315392\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.434797287\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.212438583\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████▏  | 107/150 [1:12:48<07:32, 10.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 106 ------------------------------\n",
            "rewards/mean:\t8.307323456\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.490515709\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.607261658\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 108/150 [1:12:59<07:30, 10.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 107 ------------------------------\n",
            "rewards/mean:\t8.827028275\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.901086330\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.634393692\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 109/150 [1:13:10<07:24, 10.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 108 ------------------------------\n",
            "rewards/mean:\t8.558795929\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.742140293\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.884517670\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 110/150 [1:13:23<07:33, 11.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 109 ------------------------------\n",
            "rewards/mean:\t9.004119873\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.984146118\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.670726776\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 111/150 [1:13:35<07:26, 11.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 110 ------------------------------\n",
            "rewards/mean:\t8.992073059\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.995517254\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.626668930\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▍  | 112/150 [1:13:47<07:28, 11.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 111 ------------------------------\n",
            "rewards/mean:\t8.883487701\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.636909008\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.775672913\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:1246: UserWarning: The average ratio of batch (10.45) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            " 75%|███████▌  | 113/150 [1:13:58<07:09, 11.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 112 ------------------------------\n",
            "rewards/mean:\t8.468021393\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.625261307\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.878322601\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 114/150 [1:14:09<06:41, 11.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 113 ------------------------------\n",
            "rewards/mean:\t8.893858910\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.872673988\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.872364044\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 115/150 [1:14:21<06:41, 11.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 114 ------------------------------\n",
            "rewards/mean:\t8.804882050\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.907965183\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.900264740\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 116/150 [1:14:33<06:35, 11.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 115 ------------------------------\n",
            "rewards/mean:\t8.737190247\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.641550064\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.748294830\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 117/150 [1:14:43<06:08, 11.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 116 ------------------------------\n",
            "rewards/mean:\t9.202364922\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t6.145123482\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.269033432\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 79%|███████▊  | 118/150 [1:14:53<05:49, 10.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 117 ------------------------------\n",
            "rewards/mean:\t9.065437317\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t6.097914696\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.080053329\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 79%|███████▉  | 119/150 [1:15:04<05:37, 10.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 118 ------------------------------\n",
            "rewards/mean:\t9.050843239\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.934152603\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.318870544\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 120/150 [1:15:13<05:06, 10.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 119 ------------------------------\n",
            "rewards/mean:\t9.056337357\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.853156090\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.259372711\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 121/150 [1:15:23<05:00, 10.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 120 ------------------------------\n",
            "rewards/mean:\t8.746772766\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.951889038\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t16.650207520\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████▏ | 122/150 [1:15:33<04:44, 10.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 121 ------------------------------\n",
            "rewards/mean:\t8.749676704\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.556866646\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.612552643\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▏ | 123/150 [1:15:42<04:27,  9.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 122 ------------------------------\n",
            "rewards/mean:\t8.816420555\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.799846649\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.597343445\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 124/150 [1:15:52<04:17,  9.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 123 ------------------------------\n",
            "rewards/mean:\t9.144370079\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t6.002354622\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.150920868\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 125/150 [1:16:02<04:08,  9.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 124 ------------------------------\n",
            "rewards/mean:\t9.062717438\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t6.162988663\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.775766373\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 126/150 [1:16:13<04:06, 10.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 125 ------------------------------\n",
            "rewards/mean:\t8.949964523\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.988226414\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.959266663\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▍ | 127/150 [1:16:23<03:53, 10.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 126 ------------------------------\n",
            "rewards/mean:\t8.624000549\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.950284004\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t15.619895935\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 128/150 [1:16:31<03:30,  9.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 127 ------------------------------\n",
            "rewards/mean:\t8.836909294\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.939373016\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.447742462\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 129/150 [1:16:40<03:17,  9.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 128 ------------------------------\n",
            "rewards/mean:\t9.099449158\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t6.016192436\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.592924118\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 130/150 [1:16:49<03:02,  9.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 129 ------------------------------\n",
            "rewards/mean:\t8.448755264\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.545903683\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.062114716\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 131/150 [1:16:59<02:57,  9.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 130 ------------------------------\n",
            "rewards/mean:\t9.111933708\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.675228119\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.379676819\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 132/150 [1:17:08<02:48,  9.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 131 ------------------------------\n",
            "rewards/mean:\t8.012901306\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.237126827\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t16.897949219\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%|████████▊ | 133/150 [1:17:19<02:45,  9.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 132 ------------------------------\n",
            "rewards/mean:\t8.335606575\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.621912956\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t16.385120392\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%|████████▉ | 134/150 [1:17:27<02:30,  9.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 133 ------------------------------\n",
            "rewards/mean:\t8.984245300\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.878456593\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.494020462\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 135/150 [1:17:37<02:23,  9.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 134 ------------------------------\n",
            "rewards/mean:\t9.093730927\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t6.026989460\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.949279785\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 136/150 [1:17:46<02:10,  9.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 135 ------------------------------\n",
            "rewards/mean:\t9.045806885\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t6.056615829\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.541975021\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|█████████▏| 137/150 [1:17:56<02:05,  9.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 136 ------------------------------\n",
            "rewards/mean:\t8.879863739\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t6.039972305\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.107030869\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 138/150 [1:18:06<01:57,  9.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 137 ------------------------------\n",
            "rewards/mean:\t9.041217804\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.843196392\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.426288605\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 139/150 [1:18:17<01:48,  9.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 138 ------------------------------\n",
            "rewards/mean:\t8.676990509\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.659066677\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.511442184\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 140/150 [1:18:27<01:39,  9.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 139 ------------------------------\n",
            "rewards/mean:\t8.637359619\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.639023781\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.044101715\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|█████████▍| 141/150 [1:18:37<01:30, 10.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 140 ------------------------------\n",
            "rewards/mean:\t8.217430115\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t4.937016964\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t18.448354721\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▍| 142/150 [1:18:46<01:17,  9.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 141 ------------------------------\n",
            "rewards/mean:\t8.585564613\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.784340858\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.646154404\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 143/150 [1:18:55<01:07,  9.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 142 ------------------------------\n",
            "rewards/mean:\t8.814210892\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.671211243\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.706325531\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 144/150 [1:19:07<01:00, 10.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 143 ------------------------------\n",
            "rewards/mean:\t8.581809998\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.755835533\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.868570328\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 145/150 [1:19:17<00:50, 10.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 144 ------------------------------\n",
            "rewards/mean:\t8.982486725\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t6.041881561\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t17.811870575\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 146/150 [1:19:29<00:42, 10.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 145 ------------------------------\n",
            "rewards/mean:\t8.912397385\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t6.029584885\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t16.172554016\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 147/150 [1:19:40<00:32, 10.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 146 ------------------------------\n",
            "rewards/mean:\t8.791687012\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.812608719\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.496477127\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 99%|█████████▊| 148/150 [1:19:51<00:22, 11.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 147 ------------------------------\n",
            "rewards/mean:\t8.776826859\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.799866676\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.845817566\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 149/150 [1:20:04<00:11, 11.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 148 ------------------------------\n",
            "rewards/mean:\t9.053957939\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t6.131712914\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t19.449970245\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [1:20:17<00:00, 32.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------ STEP 149 ------------------------------\n",
            "rewards/mean:\t8.810099602\t<---- average reward over this batch (higher=better, noisy)\n",
            "ppo/returns/mean:\t5.674315453\t<---- model-estimated average discounted reward\n",
            "objective/kl:\t20.663236618\t<---- how far we are from the original model (regularizer)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "max_steps = 150   # can be insufficient for some tasks - watch your learning curves\n",
        "generation_kwargs = dict(\n",
        "    min_length=-1, max_new_tokens=128, do_sample=True, top_k=0, top_p=1.0, pad_token_id=main_tokenizer.eos_token_id)\n",
        "#                                  ^-- task-specific parameter!\n",
        "with tqdm(enumerate(ppo_trainer.dataloader), total=max_steps) as progressbar:\n",
        "  # note: ppo_trainer.dataloader is just a regular dataloader of queries, no RL-specific magic :)\n",
        "  for epoch, batch in progressbar:\n",
        "    if epoch >= max_steps:\n",
        "        break\n",
        "    # Rollout stage: generate continuations from batch queries using main_model\n",
        "    response_tensors = ppo_trainer.generate(batch['input_ids'], **generation_kwargs)\n",
        "    # ^-- list of tensors of token ids from main model tokenizer\n",
        "    # de-tokenize responses to strings (since reward model uses a different tokenizer)\n",
        "    batch[\"response\"] = [main_tokenizer.decode(response.squeeze()) for response in response_tensors]\n",
        "    # note: response_tensors already contain query tokens, so we don't need to add queries manually.\n",
        "    # This may not be true for other tasks: check this manually by viewing batch[\"response\"] and batch[\"query\"]\n",
        "    # Evaluation stage\n",
        "    rewards = compute_reward(batch['response'])\n",
        "    # Update stage\n",
        "    stats = ppo_trainer.step(batch['input_ids'], response_tensors, list(rewards.split(1)))\n",
        "    stats['rewards/mean'] = rewards.mean().item()\n",
        "\n",
        "    print(\"-\" * 30, 'STEP', epoch, '-' * 30)\n",
        "    print(f'rewards/mean:\\t{stats[\"rewards/mean\"]:.9f}\\t<---- average reward over this batch (higher=better, noisy)')\n",
        "    print(f'ppo/returns/mean:\\t{stats[\"ppo/returns/mean\"]:.9f}\\t<---- model-estimated average discounted reward')\n",
        "    print(f'objective/kl:\\t{stats[\"objective/kl\"]:.9f}\\t<---- how far we are from the original model (regularizer)')\n",
        "    print()\n",
        "    if epoch % 10 == 0:\n",
        "      for name, param in main_model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            torch.save(param,f\"ft/{name}\")\n",
        "    ppo_trainer.log_stats(stats, batch, list(rewards.split(1)))\n",
        "  for name, param in main_model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "          torch.save(param,f\"ft/{name}\")\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 3,663,429 || all params: 777,186,629 || trainable%: 0.4714\n"
          ]
        }
      ],
      "source": [
        "import trl\n",
        "import torch\n",
        "import transformers\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "qconfig = transformers.BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_use_double_quant=True)\n",
        "main_tokenizer = transformers.AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")\n",
        "main_tokenizer.pad_token = main_tokenizer.eos_token\n",
        "main_model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(\"openai-community/gpt2-large\", device_map=device,quantization_config = qconfig,torch_dtype = torch.float32)\n",
        "\n",
        "from peft import get_peft_model,LoraConfig,TaskType\n",
        "lora_config = LoraConfig(\n",
        "                target_modules=[\"c_attn\",'c_proj','c_fc','lm_head'],\n",
        "                lora_alpha=32,\n",
        "                inference_mode=False,\n",
        "                use_rslora = True,\n",
        "                bias = 'all',\n",
        "                lora_dropout=0.,\n",
        "                r=4,\n",
        "                task_type=TaskType.CAUSAL_LM,\n",
        "                init_lora_weights='gaussian'\n",
        "            )\n",
        "main_model = get_peft_model(main_model,lora_config)\n",
        "main_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_16416\\1397046154.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  param.copy_(torch.load(f\"ft/{name}\"))\n"
          ]
        }
      ],
      "source": [
        "for name, param in main_model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        param.requires_grad_(False)\n",
        "        param.copy_(torch.load(f\"ft/{name}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "you know of the way you want to masturbate. 9.679192543029785\n",
            "you know I love kissing girls. 9.867589950561523\n",
            "you know her penis. 9.499441146850586\n",
            "you know how sex is. It's an orgasm. 9.90887451171875\n",
            "you know where it was that got sex. 9.739014625549316\n",
            "you know what the woman said. This wasn't sex.\" 9.586249351501465\n",
            "you know, with sexual intercourse, sex, that is it. 9.430999755859375\n",
            "you know that you have sex. 9.84558391571045\n",
            "you know what they're used for. They're good quality sex. 9.89096736907959\n",
            "you know what will make the girl have sex. 9.96318244934082\n",
            "you know him as the adult male.\" 9.542046546936035\n",
            "you know how she wants her panties to feel. 9.84951400756836\n",
            "you know how sexually sexy you look. 9.829161643981934\n",
            "you know the reason I want to fuck her. 9.712608337402344\n",
            "you know. Do something. Let them fuck, now. 8.319439888000488\n"
          ]
        }
      ],
      "source": [
        "gens = main_model.model.pretrained_model.generate(**main_tokenizer([\"you know\"]*15,return_tensors='pt'), max_new_tokens=50, do_sample=True)\n",
        "\n",
        "for i in range(15):\n",
        "    eos_idx = gens[i].tolist().index(main_tokenizer.pad_token_id)\n",
        "    gen = main_tokenizer.decode(gens[i,:eos_idx].flatten().detach().cpu().tolist())\n",
        "    reward = reward_model(reward_tokenizer(gen,return_tensors='pt')['input_ids'].cuda())['logits'][0,0].item()\n",
        "    print(gen,reward)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "4c8ff454cd947027f86954d72bf940c689a97dcc494eb53cfe4813862c6065fe"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1692cc1532df4c2695c729a964a31da8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23db3f8d31cc4c5d98af465767af45af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ca9e640d5d549658e42fd73af8ea399": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "411cd47238a94edc8283e178e04d386f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44f3d7c434354a90bfa3d4750048b80f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4922d11311b846f59278623ec5b6dd39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ad4da252cb64e818d35eb3869adc81c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1692cc1532df4c2695c729a964a31da8",
            "max": 24895,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55c624445ca44090a2f109d3bf5f3f6a",
            "value": 24895
          }
        },
        "55c624445ca44090a2f109d3bf5f3f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5cdc9539cca3499abb4958d40142d77c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63e252cfd9f44ef79137473b618828dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ca9e640d5d549658e42fd73af8ea399",
            "placeholder": "​",
            "style": "IPY_MODEL_c8ea1c2d3b5040378d0f2bd213933603",
            "value": "Map: 100%"
          }
        },
        "6d3c3f5bfad345f68b6e62ab870e69bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74b0875944e047e6a4d09cc988013ae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc859c36e95646b78e9a08111ce1735b",
              "IPY_MODEL_98156b41fab9493cac7eb8edfd1f611a",
              "IPY_MODEL_f0adf0ab77d74ff3857ffa5b9b1a0373"
            ],
            "layout": "IPY_MODEL_6d3c3f5bfad345f68b6e62ab870e69bf"
          }
        },
        "9604bff7c9414071a55d063fdf4174fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98156b41fab9493cac7eb8edfd1f611a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23db3f8d31cc4c5d98af465767af45af",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4922d11311b846f59278623ec5b6dd39",
            "value": 39
          }
        },
        "a984b403d0464e88b4539d586dfc4cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc859c36e95646b78e9a08111ce1735b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cdc9539cca3499abb4958d40142d77c",
            "placeholder": "​",
            "style": "IPY_MODEL_a984b403d0464e88b4539d586dfc4cc2",
            "value": " 78%"
          }
        },
        "c270953012ea437fa8ff299292a41837": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c487b0154d434955ba8070253af01e1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8ea1c2d3b5040378d0f2bd213933603": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3a8dcfad53b4de885b39ee83e6029ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_411cd47238a94edc8283e178e04d386f",
            "placeholder": "​",
            "style": "IPY_MODEL_9604bff7c9414071a55d063fdf4174fa",
            "value": " 24895/24895 [00:43&lt;00:00, 568.24 examples/s]"
          }
        },
        "f0adf0ab77d74ff3857ffa5b9b1a0373": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c487b0154d434955ba8070253af01e1c",
            "placeholder": "​",
            "style": "IPY_MODEL_c270953012ea437fa8ff299292a41837",
            "value": " 39/50 [51:09&lt;14:44, 80.38s/it]"
          }
        },
        "f6ba50eafdeb4b94a1e7c22c5027f709": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63e252cfd9f44ef79137473b618828dd",
              "IPY_MODEL_4ad4da252cb64e818d35eb3869adc81c",
              "IPY_MODEL_d3a8dcfad53b4de885b39ee83e6029ef"
            ],
            "layout": "IPY_MODEL_44f3d7c434354a90bfa3d4750048b80f"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
